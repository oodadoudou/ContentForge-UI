#!/usr/bin/python
# -*- coding: utf-8 -*-
import sys
import os
import logging
import re
import uuid
import json
import shutil
import tempfile
from posixpath import normpath
from zipfile import ZipFile, ZIP_STORED, ZIP_DEFLATED
from xml.dom.minidom import parseString, getDOMImplementation

# --- Compatibility & Setup ---
try:
    from urllib.parse import unquote
except ImportError:
    from urllib import unquote

# --- Logging Setup ---
logger = logging.getLogger(__name__)
loghandler = logging.StreamHandler()
logformatter = logging.Formatter("%(levelname)s: %(message)s")
loghandler.setFormatter(logformatter)
if not logger.handlers:
    logger.addHandler(loghandler)
logger.setLevel(logging.INFO)

# --- Helper Functions ---
def new_tag(dom, name, attrs=None, text=None):
    """Creates a new XML element."""
    tag = dom.createElement(name)
    if attrs:
        for attr, value in attrs.items():
            tag.setAttribute(attr, value)
    if text:
        tag.appendChild(dom.createTextNode(str(text)))
    return tag

def get_path_part(path):
    """Gets the directory part of a file path."""
    return os.path.dirname(path)

def natural_sort_key(s):
    """Key for natural sorting (e.g., 'vol 2' before 'vol 10')."""
    return [int(c) if c.isdigit() else c.lower() for c in re.split('([0-9]+)', s)]

def sanitize_filename(name):
    """Removes illegal characters from a string for use as a filename."""
    return re.sub(r'[<>:"/\\|?*]', '', name).strip()

def load_default_path():
    """
    ä»å…±äº«è®¾ç½®æ–‡ä»¶ä¸­è¯»å–é»˜è®¤å·¥ä½œç›®å½•ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›ç”¨æˆ·ä¸‹è½½æ–‡ä»¶å¤¹ã€‚
    é€»è¾‘å‚è€ƒè‡ªç”¨æˆ·æä¾›çš„ epub_toolkit.pyã€‚
    """
    try:
        # å‡è®¾é¡¹ç›®ç»“æ„ä¸º .../ProjectRoot/scripts/script.py
        # ä¸”è®¾ç½®æ–‡ä»¶ä½äº .../ProjectRoot/shared_assets/settings.json
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        settings_path = os.path.join(project_root, 'shared_assets', 'settings.json')
        if os.path.exists(settings_path):
            with open(settings_path, 'r', encoding='utf-8') as f:
                settings = json.load(f)
            default_dir = settings.get("default_work_dir")
            # å¦‚æœè·¯å¾„å­˜åœ¨ä¸”æ˜¯æœ‰æ•ˆç›®å½•ï¼Œåˆ™è¿”å›è¯¥è·¯å¾„
            if default_dir and os.path.isdir(default_dir):
                return default_dir
    except Exception as e:
        # åœ¨ debug æ¨¡å¼ä¸‹è®°å½•é”™è¯¯ï¼Œä½†ä¸ä¸­æ–­ç¨‹åº
        logger.debug(f"è¯»å–å…±äº«è®¾ç½®å¤±è´¥: {e}")
    
    # æœ€ç»ˆå›é€€åˆ°ç”¨æˆ·çš„ä¸‹è½½æ–‡ä»¶å¤¹
    return os.path.join(os.path.expanduser("~"), "Downloads")

class EpubMerger:
    """
    A class that merges multiple EPUB files by unpacking them, consolidating
    their contents, and repacking them into a single, clean EPUB file.
    This approach is more robust than in-memory manipulation.
    """
    def __init__(self, input_files, output_path, title_opt=None):
        self.input_files = input_files
        self.output_path = output_path
        self.title_opt = title_opt
        self.temp_dir = tempfile.mkdtemp(prefix="epub_merge_")
        self.book_dirs = []
        self.merged_content_dir = os.path.join(self.temp_dir, "merged_OEBPS")
        os.makedirs(self.merged_content_dir, exist_ok=True)
        
        # Data accumulated from all books
        self.manifest_items = []
        self.spine_items = []
        self.toc_navpoints = []
        self.metadata = {
            "titles": [],
            "creators": set(),
            "cover_full_path": None, # Absolute path in temp dir
            "cover_href": None,      # Original href for extension
            "cover_media_type": None
        }

    def run(self):
        """Executes the entire merge process."""
        try:
            logger.info(f"åˆ›å»ºä¸´æ—¶å·¥ä½œç›®å½•: {self.temp_dir}")
            
            self._unpack_all()
            self._process_books()
            self._repack_epub()
            
            logger.info(f"ğŸ‰ åˆå¹¶æˆåŠŸ! æ–‡ä»¶ä¿å­˜åœ¨: {self.output_path}")

        except Exception as e:
            logger.error(f"åˆå¹¶è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        finally:
            logger.info("æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
            shutil.rmtree(self.temp_dir)

    def _unpack_all(self):
        """Unpacks each input EPUB into its own subdirectory."""
        for i, epub_path in enumerate(self.input_files):
            book_dir = os.path.join(self.temp_dir, f"book_{i+1}")
            os.makedirs(book_dir, exist_ok=True)
            self.book_dirs.append(book_dir)
            logger.info(f"è§£åŒ… ({i+1}/{len(self.input_files)}): {os.path.basename(epub_path)} -> {os.path.basename(book_dir)}/")
            with ZipFile(epub_path, 'r') as zip_ref:
                zip_ref.extractall(book_dir)

    def _process_books(self):
        """Processes each unpacked book directory to gather data and copy files."""
        for i, book_dir in enumerate(self.book_dirs):
            book_num = i + 1
            logger.info(f"å¤„ç†è§£åŒ…åçš„ä¹¦ç± #{book_num}...")
            
            try:
                container_path = os.path.join(book_dir, "META-INF", "container.xml")
                if not os.path.exists(container_path):
                    logger.warning(f"  ä¹¦ç± #{book_num} ç¼ºå°‘ container.xmlï¼Œè·³è¿‡æ­¤ä¹¦ã€‚")
                    continue
                
                with open(container_path, 'rb') as f:
                    container_dom = parseString(f.read())
                
                # Use getElementsByTagNameNS to be namespace-agnostic
                rootfile_path = container_dom.getElementsByTagNameNS("*", "rootfile")[0].getAttribute("full-path")
                opf_path = os.path.join(book_dir, rootfile_path)
                opf_dir = get_path_part(opf_path)

                with open(opf_path, 'rb') as f:
                    opf_dom = parseString(f.read())

                self._gather_metadata(opf_dom, opf_dir, book_num)
                self._process_manifest(opf_dom, opf_dir, book_num)
                self._process_spine(opf_dom, book_num)
                self._process_toc(opf_dom, opf_dir, book_num)

            except Exception as e:
                logger.error(f"  å¤„ç†ä¹¦ç± #{book_num} å¤±è´¥: {e}", exc_info=True)

    def _gather_metadata(self, opf_dom, opf_dir, book_num):
        """Gathers title, creator, and cover info from the OPF DOM."""
        try:
            title = opf_dom.getElementsByTagNameNS("*", "title")[0].firstChild.data
            self.metadata["titles"].append(title)
        except (IndexError, AttributeError):
            self.metadata["titles"].append(f"Book {book_num}")
        
        for creator_node in opf_dom.getElementsByTagNameNS("*", "creator"):
            if creator_node.firstChild:
                self.metadata["creators"].add(creator_node.firstChild.data)

        # Robust cover handling, only from the first book
        if book_num == 1:
            cover_meta = next((m for m in opf_dom.getElementsByTagNameNS("*", "meta") if m.getAttribute("name") == "cover"), None)
            if cover_meta:
                cover_id = cover_meta.getAttribute("content")
                manifest_items = opf_dom.getElementsByTagNameNS("*", "item")
                cover_item = next((i for i in manifest_items if i.getAttribute("id") == cover_id), None)
                if cover_item:
                    href = unquote(cover_item.getAttribute("href"))
                    full_src_path = os.path.join(opf_dir, href)
                    if os.path.exists(full_src_path):
                        self.metadata["cover_full_path"] = full_src_path
                        self.metadata["cover_href"] = href
                        self.metadata["cover_media_type"] = cover_item.getAttribute("media-type")
                        logger.info(f"  æ‰¾åˆ°å°é¢å›¾ç‰‡: {href}")
                    else:
                        logger.warning(f"  å°é¢åœ¨ manifest ä¸­å£°æ˜ï¼Œä½†æ–‡ä»¶æœªæ‰¾åˆ°: {full_src_path}")

    def _process_manifest(self, opf_dom, opf_dir, book_num):
        """Copies files and creates new manifest entries."""
        for item in opf_dom.getElementsByTagNameNS("*", "item"):
            item_id = item.getAttribute("id")
            href_orig = unquote(item.getAttribute("href"))
            media_type = item.getAttribute("media-type")

            if media_type == "application/x-dtbncx+xml":
                continue

            src_path = os.path.join(opf_dir, href_orig)
            href_new = normpath(os.path.join(str(book_num), href_orig))
            dest_path = os.path.join(self.merged_content_dir, href_new)
            
            os.makedirs(get_path_part(dest_path), exist_ok=True)
            
            if os.path.exists(src_path):
                shutil.copy2(src_path, dest_path)
                new_id = f"b{book_num}_{item_id}"
                self.manifest_items.append({"id": new_id, "href": href_new, "media-type": media_type})
            else:
                logger.warning(f"  æ–‡ä»¶åœ¨ manifest ä¸­å£°æ˜ä½†æœªæ‰¾åˆ°: {src_path}")
    
    def _process_spine(self, opf_dom, book_num):
        """Gathers spine items."""
        for itemref in opf_dom.getElementsByTagNameNS("*", "itemref"):
            idref = itemref.getAttribute("idref")
            new_idref = f"b{book_num}_{idref}"
            self.spine_items.append(new_idref)

    def _process_toc(self, opf_dom, opf_dir, book_num):
        """Gathers and adapts TOC entries."""
        manifest_items = opf_dom.getElementsByTagNameNS("*", "item")
        ncx_item = next((i for i in manifest_items if i.getAttribute("media-type") == "application/x-dtbncx+xml"), None)
        
        book_title = self.metadata["titles"][-1]
        book_navpoint = {"label": book_title, "id": f"book_toc_{book_num}", "children": []}

        first_spine_id = next((s.getAttribute("idref") for s in opf_dom.getElementsByTagNameNS("*", "itemref")), None)
        if first_spine_id:
            first_item = next((i for i in manifest_items if i.getAttribute("id") == first_spine_id), None)
            if first_item:
                first_href = unquote(first_item.getAttribute("href"))
                book_navpoint["src"] = normpath(os.path.join(str(book_num), first_href))

        if not ncx_item:
            logger.warning(f"  ä¹¦ç± #{book_num} æœªæ‰¾åˆ° toc.ncx å£°æ˜ã€‚")
            self.toc_navpoints.append(book_navpoint)
            return

        ncx_path = os.path.join(opf_dir, unquote(ncx_item.getAttribute("href")))
        if not os.path.exists(ncx_path):
            logger.warning(f"  toc.ncx æ–‡ä»¶åœ¨ manifest ä¸­å£°æ˜ä½†æœªæ‰¾åˆ°: {ncx_path}")
            self.toc_navpoints.append(book_navpoint)
            return
        
        with open(ncx_path, 'rb') as f:
            ncx_dom = parseString(f.read())
        
        ncx_dir = get_path_part(ncx_path)

        for navpoint in ncx_dom.getElementsByTagNameNS("*", "navPoint"):
            if navpoint.parentNode.tagName == "navMap":
                child = self._parse_navpoint(navpoint, book_num, ncx_dir, opf_dir)
                book_navpoint["children"].append(child)
        
        self.toc_navpoints.append(book_navpoint)

    def _parse_navpoint(self, navpoint_node, book_num, ncx_dir, opf_dir):
        """Recursively parses a navPoint XML node into a dictionary."""
        label = navpoint_node.getElementsByTagNameNS("*", "text")[0].firstChild.data
        content_node = navpoint_node.getElementsByTagNameNS("*", "content")[0]
        src_orig = unquote(content_node.getAttribute("src"))
        
        # Correctly resolve the content path relative to the OPF file
        src_abs_path = os.path.abspath(os.path.join(ncx_dir, src_orig))
        src_rel_to_opf = os.path.relpath(src_abs_path, opf_dir)
        src_new = normpath(os.path.join(str(book_num), src_rel_to_opf))
        
        nav_dict = {
            "id": f"b{book_num}_{navpoint_node.getAttribute('id')}",
            "label": label,
            "src": src_new,
            "children": []
        }
        
        for child_node in navpoint_node.childNodes:
            if child_node.nodeType == child_node.ELEMENT_NODE and child_node.tagName == "navPoint":
                nav_dict["children"].append(self._parse_navpoint(child_node, book_num, ncx_dir, opf_dir))
        
        return nav_dict

    def _repack_epub(self):
        """Builds the final OPF, NCX, and zips everything into a new EPUB."""
        logger.info("æ„å»ºæœ€ç»ˆçš„ EPUB æ–‡ä»¶...")
        
        if self.metadata.get("cover_full_path"):
            cover_src_path = self.metadata["cover_full_path"]
            cover_orig_href = self.metadata["cover_href"]
            cover_dest_href = "cover" + os.path.splitext(cover_orig_href)[1]
            cover_dest_path = os.path.join(self.merged_content_dir, cover_dest_href)
            shutil.copy2(cover_src_path, cover_dest_path)

            self.manifest_items.insert(0, {"id": "cover-img", "href": cover_dest_href, "media-type": self.metadata["cover_media_type"]})
            self.manifest_items.insert(1, {"id": "cover-page", "href": "cover.xhtml", "media-type": "application/xhtml+xml"})
            self.spine_items.insert(0, "cover-page")

            cover_xhtml = f"""<?xml version="1.0" encoding="UTF-8"?>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head><title>Cover</title><style type="text/css">body{{margin:0;padding:0;text-align:center;}}img{{max-width:100%;max-height:100vh;}}</style></head>
<body><div><img src="{cover_dest_href}" alt="Cover"/></div></body></html>"""
            with open(os.path.join(self.merged_content_dir, "cover.xhtml"), "w", encoding="utf-8") as f:
                f.write(cover_xhtml)

        unique_id = f"urn:uuid:{uuid.uuid4()}"
        self._write_opf(unique_id)
        self._write_ncx(unique_id)

        with ZipFile(self.output_path, 'w', ZIP_DEFLATED, allowZip64=True) as zf:
            zf.writestr("mimetype", "application/epub+zip", ZIP_STORED)
            
            container_xml = """<?xml version="1.0" encoding="UTF-8"?>
<container version="1.0" xmlns="urn:oasis:names:tc:opendocument:xmlns:container">
  <rootfiles>
    <rootfile full-path="OEBPS/content.opf" media-type="application/oebps-package+xml"/>
  </rootfiles>
</container>"""
            zf.writestr("META-INF/container.xml", container_xml)
            
            for root, _, files in os.walk(self.merged_content_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    archive_name = os.path.join("OEBPS", os.path.relpath(file_path, self.merged_content_dir))
                    zf.write(file_path, archive_name)

    def _write_opf(self, unique_id):
        """Writes the content.opf file."""
        dom = getDOMImplementation().createDocument(None, "package", None)
        package = dom.documentElement
        package.setAttribute("version", "2.0")
        package.setAttribute("xmlns", "http://www.idpf.org/2007/opf")
        package.setAttribute("unique-identifier", "bookid")

        metadata = new_tag(dom, "metadata", attrs={"xmlns:dc": "http://purl.org/dc/elements/1.1/", "xmlns:opf": "http://www.idpf.org/2007/opf"})
        package.appendChild(metadata)
        metadata.appendChild(new_tag(dom, "dc:identifier", text=unique_id, attrs={"id": "bookid"}))
        
        # Use the user-provided title if available, otherwise generate one.
        final_title = self.title_opt if self.title_opt else f"{self.metadata['titles'][0]} (Merged)"
        metadata.appendChild(new_tag(dom, "dc:title", text=final_title))

        for creator in sorted(list(self.metadata["creators"])):
            metadata.appendChild(new_tag(dom, "dc:creator", text=creator))
        if self.metadata.get("cover_full_path"):
            metadata.appendChild(new_tag(dom, "meta", attrs={"name": "cover", "content": "cover-img"}))

        manifest = new_tag(dom, "manifest")
        package.appendChild(manifest)
        manifest.appendChild(new_tag(dom, "item", attrs={"id": "ncx", "href": "toc.ncx", "media-type": "application/x-dtbncx+xml"}))
        for item in self.manifest_items:
            manifest.appendChild(new_tag(dom, "item", attrs=item))

        spine = new_tag(dom, "spine", attrs={"toc": "ncx"})
        package.appendChild(spine)
        for idref in self.spine_items:
            spine.appendChild(new_tag(dom, "itemref", attrs={"idref": idref}))

        opf_content = dom.toprettyxml(indent="  ", encoding="utf-8")
        with open(os.path.join(self.merged_content_dir, "content.opf"), "wb") as f:
            f.write(opf_content)

    def _write_ncx(self, unique_id):
        """Writes the toc.ncx file."""
        dom = getDOMImplementation().createDocument(None, "ncx", None)
        ncx = dom.documentElement
        ncx.setAttribute("version", "2.0")
        ncx.setAttribute("xmlns", "http://www.daisy.org/z3986/2005/ncx/")
        
        head = new_tag(dom, "head")
        head.appendChild(new_tag(dom, "meta", attrs={"name": "dtb:uid", "content": unique_id}))
        ncx.appendChild(head)
        
        doc_title = new_tag(dom, "docTitle")
        # Use the user-provided title here as well.
        final_title = self.title_opt if self.title_opt else f"{self.metadata['titles'][0]} (Merged)"
        doc_title.appendChild(new_tag(dom, "text", text=final_title))
        ncx.appendChild(doc_title)
        
        navmap = new_tag(dom, "navMap")
        ncx.appendChild(navmap)
        
        play_order = 1
        
        if self.metadata.get("cover_full_path"):
            cover_nav = new_tag(dom, "navPoint", attrs={"id": "nav-cover", "playOrder": str(play_order)})
            play_order += 1
            nav_label = new_tag(dom, "navLabel")
            nav_label.appendChild(new_tag(dom, "text", text="å°é¢"))
            cover_nav.appendChild(nav_label)
            cover_nav.appendChild(new_tag(dom, "content", attrs={"src": "cover.xhtml"}))
            navmap.appendChild(cover_nav)

        def create_nav_points(parent_node, nav_dicts):
            nonlocal play_order
            for nav_dict in nav_dicts:
                navpoint = new_tag(dom, "navPoint", attrs={"id": nav_dict["id"], "playOrder": str(play_order)})
                play_order += 1
                nav_label = new_tag(dom, "navLabel")
                nav_label.appendChild(new_tag(dom, "text", text=nav_dict["label"]))
                navpoint.appendChild(nav_label)
                if nav_dict.get("src"):
                    navpoint.appendChild(new_tag(dom, "content", attrs={"src": nav_dict["src"]}))
                
                if nav_dict["children"]:
                    create_nav_points(navpoint, nav_dict["children"])
                
                parent_node.appendChild(navpoint)

        create_nav_points(navmap, self.toc_navpoints)

        ncx_content = dom.toprettyxml(indent="  ", encoding="utf-8")
        with open(os.path.join(self.merged_content_dir, "toc.ncx"), "wb") as f:
            f.write(ncx_content)

# --- UI and Execution ---
def run_epub_merge_ui():
    """Main function to run the tool with user interaction."""
    print("=========================================================")
    print("=      å¢å¼ºç‰ˆ EPUB åˆå¹¶å·¥å…· (è§£åŒ…-æ•´åˆ-æ‰“åŒ…)      =")
    print("=========================================================")
    print("åŠŸèƒ½ï¼šå°†æŒ‡å®šæ–‡ä»¶å¤¹å†…çš„æ‰€æœ‰ EPUB æ–‡ä»¶å½»åº•è§£åŒ…åé‡æ–°åˆå¹¶ã€‚")
    print("      æ­¤æ–¹æ³•æ›´ç¨³å®šï¼Œèƒ½æ›´å¥½åœ°å¤„ç†ä¸åŒç»“æ„çš„ EPUB æ–‡ä»¶ã€‚")

    # --- åŠŸèƒ½æ›´æ–° 1: é»˜è®¤å¼€å¯ DEBUG æ¨¡å¼ ---
    logger.setLevel(logging.DEBUG)
    print("\n--- è¯¦ç»†æ—¥å¿— (DEBUG æ¨¡å¼) å·²é»˜è®¤å¯ç”¨ ---")

    # --- åŠŸèƒ½æ›´æ–° 3: ä½¿ç”¨æ–°çš„é»˜è®¤è·¯å¾„è¯»å–é€»è¾‘ ---
    default_path = load_default_path()
    input_directory = input(f"\nè¯·è¾“å…¥ EPUB æ–‡ä»¶å¤¹è·¯å¾„ (é»˜è®¤ä¸º: {default_path}): ").strip() or default_path

    if not os.path.isdir(input_directory):
        sys.exit(f"\né”™è¯¯ï¼šæ–‡ä»¶å¤¹ '{input_directory}' ä¸å­˜åœ¨ã€‚")

    all_epub_files = sorted(
        [f for f in os.listdir(input_directory) if f.lower().endswith('.epub')],
        key=natural_sort_key
    )
    
    output_filename_input = input("\nè¯·è¾“å…¥æ–° EPUB çš„æ–‡ä»¶å (ä¾‹å¦‚: æˆ‘çš„åˆé›†.epub): ").strip()
    output_filename = sanitize_filename(output_filename_input) if output_filename_input else "merged_epubs.epub"
    if not output_filename.lower().endswith('.epub'):
        output_filename += '.epub'
    
    # ä»æ–‡ä»¶åä¸­æå–ä¹¦åï¼Œç”¨äºå…ƒæ•°æ®
    new_epub_title = os.path.splitext(output_filename)[0]
    
    files_to_merge_names = [f for f in all_epub_files if f != output_filename]

    if not files_to_merge_names:
        sys.exit(f"\né”™è¯¯: åœ¨ '{input_directory}' ä¸­æ²¡æœ‰æ‰¾åˆ°å¯ä¾›åˆå¹¶çš„ EPUB æ–‡ä»¶ã€‚")

    # --- åŠŸèƒ½æ›´æ–° 2: äº¤äº’å¼æ’åº ---
    while True:
        print("\n" + "="*80)
        print(f"  æ£€æµ‹åˆ°ä»¥ä¸‹æ–‡ä»¶ï¼Œå°†æŒ‰æ­¤é¡ºåºåˆå¹¶ (å…± {len(files_to_merge_names)} ä¸ª):")
        print("-"*80)
        print(f"  {'åºå·':<4} | æ–‡ä»¶å")
        print(f"  {'-'*4} | {'-'*73}")
        for i, filename in enumerate(files_to_merge_names, 1):
            # ç›´æ¥æ‰“å°å®Œæ•´æ–‡ä»¶åï¼Œä¸å†æˆªæ–­
            print(f"  {i:<4} | {filename}")
        print("="*80)

        reorder_input = input("\næ­¤é¡ºåºæ˜¯å¦æ­£ç¡®ï¼Ÿ(Y/n): ").lower().strip()
        if reorder_input == 'n':
            print("\nè¯·è¾“å…¥æ–°çš„é¡ºåºå·ï¼Œä»¥ç©ºæ ¼åˆ†éš” (ä¾‹å¦‚: 1 3 2 4)")
            new_order_str = input("> ").strip()
            try:
                # å°†ç”¨æˆ·è¾“å…¥çš„åºå·ï¼ˆä»1å¼€å§‹ï¼‰è½¬æ¢æˆåˆ—è¡¨ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰
                new_order_indices = [int(i) - 1 for i in new_order_str.split()]
                
                # éªŒè¯ç”¨æˆ·è¾“å…¥
                if len(new_order_indices) != len(files_to_merge_names):
                    raise ValueError("è¾“å…¥çš„åºå·æ•°é‡ä¸æ–‡ä»¶æ•°é‡ä¸åŒ¹é…ã€‚")
                if len(set(new_order_indices)) != len(new_order_indices):
                    raise ValueError("è¾“å…¥çš„åºå·åŒ…å«é‡å¤æ•°å­—ã€‚")
                if not all(0 <= i < len(files_to_merge_names) for i in new_order_indices):
                     raise ValueError("è¾“å…¥çš„åºå·è¶…å‡ºäº†æœ‰æ•ˆèŒƒå›´ã€‚")
                
                # æ ¹æ®ç”¨æˆ·è¾“å…¥çš„æ–°é¡ºåºé‡æ–°æ’åˆ—æ–‡ä»¶åˆ—è¡¨
                original_list = list(files_to_merge_names)
                files_to_merge_names = [original_list[i] for i in new_order_indices]
                
                print("\nâœ… é¡ºåºå·²æ›´æ–°ã€‚")
                # å¾ªç¯å°†ç»§ç»­ï¼Œå¹¶æ˜¾ç¤ºæ–°çš„é¡ºåºä¾›ç”¨æˆ·å†æ¬¡ç¡®è®¤
            
            except ValueError as e:
                print(f"\nâŒ é”™è¯¯: {e} è¯·é‡è¯•ã€‚")
                # ç»§ç»­å¾ªç¯ï¼Œè®©ç”¨æˆ·é‡æ–°è¾“å…¥
        
        elif reorder_input in ('y', ''):
            print("\né¡ºåºå·²ç¡®è®¤ã€‚")
            break # é¡ºåºæ­£ç¡®ï¼Œè·³å‡ºå¾ªç¯
        
        else:
            print("æ— æ•ˆè¾“å…¥ï¼Œè¯·è¾“å…¥ 'y' æˆ– 'n'ã€‚")
    
    confirm = input(f"\nå³å°†å¼€å§‹åˆå¹¶ï¼Œè¾“å‡ºæ–‡ä»¶ä¸º '{output_filename}'ã€‚\nç¡®è®¤å¼€å§‹å—ï¼Ÿ(Y/n): ").lower().strip()
    if confirm == 'n':
        sys.exit("æ“ä½œå·²å–æ¶ˆã€‚")
        
    print("-" * 20)

    output_filepath = os.path.join(input_directory, output_filename)
    files_to_merge_paths = [os.path.join(input_directory, f) for f in files_to_merge_names]
    
    merger = EpubMerger(files_to_merge_paths, output_filepath, title_opt=new_epub_title)
    merger.run()

if __name__ == "__main__":
    run_epub_merge_ui()