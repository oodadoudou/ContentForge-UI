#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Diritto URL Extractor - ä» Diritto æ¦œå•é¡µé¢æå–æŒ‡å®šæ•°é‡çš„å°è¯´ URL
"""
import os
import sys
import time
import json
import argparse

# Add the diritto directory to sys.path to find browser_launcher
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from browser_launcher import setup_driver_with_auto_launch


def setup_driver():
    """é…ç½®å¹¶è¿æ¥åˆ° Chrome æµè§ˆå™¨ï¼ˆè‡ªåŠ¨å¯åŠ¨ï¼‰"""
    return setup_driver_with_auto_launch()


def extract_novel_urls(driver, page_url, count):
    """
    ä»æ¦œå•é¡µé¢æå–æŒ‡å®šæ•°é‡çš„å°è¯´ URL
    
    Args:
        driver: Selenium WebDriver å®ä¾‹
        page_url: æ¦œå•é¡µé¢ URL
        count: è¦æå–çš„å°è¯´æ•°é‡
    
    Returns:
        list: å°è¯´ URL åˆ—è¡¨
    """
    print(f"\n[ä¿¡æ¯] æ­£åœ¨è®¿é—®æ¦œå•é¡µé¢: {page_url}")
    driver.get(page_url)
    wait = WebDriverWait(driver, 30)
    
    # ç­‰å¾…é¡µé¢åŠ è½½
    time.sleep(3)
    
    print(f"[ä¿¡æ¯] æ­£åœ¨æ»šåŠ¨é¡µé¢ä»¥åŠ è½½è‡³å°‘ {count} ä¸ªå°è¯´...")
    
    # æ»šåŠ¨åŠ è½½ç­–ç•¥
    scroll_attempts = 0
    max_scroll_attempts = 20
    
    while scroll_attempts < max_scroll_attempts:
        # æŸ¥æ‰¾æ‰€æœ‰å°è¯´é“¾æ¥
        novel_links = driver.find_elements(By.CSS_SELECTOR, 'a[href*="/contents/"]')
        
        # æå–å”¯ä¸€çš„å°è¯´ URL
        unique_urls = set()
        for link in novel_links:
            href = link.get_attribute('href')
            if href and '/contents/' in href:
                # æ¸…ç† URL (ç§»é™¤æŸ¥è¯¢å‚æ•°)
                clean_url = href.split('?')[0]
                # ç¡®ä¿æ˜¯å°è¯´è¯¦æƒ…é¡µè€Œéç« èŠ‚é¡µ
                if '/episodes/' not in clean_url:
                    unique_urls.add(clean_url)
        
        current_count = len(unique_urls)
        print(f"  å½“å‰æ‰¾åˆ° {current_count} ä¸ªå”¯ä¸€å°è¯´...")
        
        # å¦‚æœå·²ç»æ‰¾åˆ°è¶³å¤Ÿçš„å°è¯´ï¼Œåœæ­¢æ»šåŠ¨
        if current_count >= count:
            print(f"âœ… å·²æ‰¾åˆ°è¶³å¤Ÿçš„å°è¯´ ({current_count} >= {count})")
            break
        
        # æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨
        last_height = driver.execute_script("return document.body.scrollHeight")
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            print(f"âš ï¸ å·²æ»šåŠ¨åˆ°åº•éƒ¨ï¼Œå…±æ‰¾åˆ° {current_count} ä¸ªå°è¯´")
            break
        
        scroll_attempts += 1
    
    # è¿”å›å‰ N ä¸ª URL
    result_urls = sorted(list(unique_urls))[:count]
    
    print(f"\nâœ… æˆåŠŸæå– {len(result_urls)} ä¸ªå°è¯´ URL")
    return result_urls


def main():
    parser = argparse.ArgumentParser(description='ä» Diritto æ¦œå•é¡µé¢æå–å°è¯´ URL')
    parser.add_argument(
        '--count',
        type=int,
        default=10,
        help='è¦æå–çš„å°è¯´æ•°é‡ (é»˜è®¤: 10)'
    )
    parser.add_argument(
        '--url',
        type=str,
        default='https://www.diritto.co.kr/explore/completed-or-published/bl?exploreSubMenu=Completed',
        help='æ¦œå•é¡µé¢ URL (é»˜è®¤: BLå®Œç»“æ¦œå•)'
    )
    
    args = parser.parse_args()
    
    # éªŒè¯æ•°é‡èŒƒå›´
    if args.count < 1 or args.count > 50:
        print("âŒ é”™è¯¯: æå–æ•°é‡å¿…é¡»åœ¨ 1-50 ä¹‹é—´")
        sys.exit(1)
    
    # è¿æ¥æµè§ˆå™¨
    driver = setup_driver()
    if not driver:
        sys.exit(1)
    
    try:
        # æå– URL
        urls = extract_novel_urls(driver, args.url, args.count)
        
        # è¾“å‡º JSON æ ¼å¼ï¼ˆä¾›å‰ç«¯ä½¿ç”¨ï¼‰
        result = {"urls": urls}
        print("\n" + "="*60)
        print("JSON è¾“å‡º:")
        print("="*60)
        print(json.dumps(result, ensure_ascii=False, indent=2))
        
        # ä¹Ÿä¿å­˜åˆ°æ–‡ä»¶ï¼ˆæ–¹ä¾¿æŸ¥çœ‹ï¼‰
        output_file = "diritto_extracted_urls.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
    except Exception as e:
        print(f"âŒ æå–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        print("\nâœ… ä»»åŠ¡å®Œæˆã€‚æµè§ˆå™¨ä¿æŒæ‰“å¼€çŠ¶æ€ã€‚")


if __name__ == "__main__":
    main()
