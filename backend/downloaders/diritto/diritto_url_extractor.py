#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Diritto URL Extractor - ä» Diritto æ¦œå•é¡µé¢æå–æŒ‡å®šæ•°é‡çš„å°è¯´ URL
"""
import os
import sys
import time
import json
import argparse

# Add the diritto directory to sys.path to find browser_launcher
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
from browser_launcher import setup_driver_with_auto_launch


def setup_driver():
    """é…ç½®å¹¶è¿æ¥åˆ° Chrome æµè§ˆå™¨ï¼ˆè‡ªåŠ¨å¯åŠ¨ï¼‰"""
    return setup_driver_with_auto_launch()


def extract_novel_urls(driver, page_url, count):
    """
    ä»æ¦œå•é¡µé¢æå–æŒ‡å®šæ•°é‡çš„å°è¯´ URL
    
    Args:
        driver: Selenium WebDriver å®ä¾‹
        page_url: æ¦œå•é¡µé¢ URL
        count: è¦æå–çš„å°è¯´æ•°é‡
    
    Returns:
        list: å°è¯´ URL åˆ—è¡¨
    """
    print(f"\n[ä¿¡æ¯] æ­£åœ¨è®¿é—®æ¦œå•é¡µé¢: {page_url}")
    driver.get(page_url)
    
    # ç­‰å¾…åˆå§‹åŠ è½½
    wait = WebDriverWait(driver, 10)
    try:
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'a[href*="/contents/"]')))
    except:
        print("âš ï¸ é¡µé¢åŠ è½½è¾ƒæ…¢æˆ–æœªæ‰¾åˆ°å°è¯´åˆ—è¡¨...")

    extracted_urls = set()
    scroll_attempts = 0
    max_scroll_attempts = 100  # å¢åŠ æœ€å¤§æ»šåŠ¨æ¬¡æ•°
    last_height = driver.execute_script("return document.body.scrollHeight")
    consecutive_no_change = 0
    max_consecutive_no_change = 5 # å…è®¸é‡è¯•çš„æ¬¡æ•°

    print(f"[ä¿¡æ¯] æ­£åœ¨æ»šåŠ¨é¡µé¢ä»¥åŠ è½½è‡³å°‘ {count} ä¸ªå°è¯´...")
    
    while scroll_attempts < max_scroll_attempts:
        # 1. æå–å½“å‰é¡µé¢ä¸Šçš„æ‰€æœ‰å°è¯´é“¾æ¥
        try:
            # Diritto çš„å°è¯´é“¾æ¥é€šå¸¸æ˜¯ /contents/æ•°å­—
            elements = driver.find_elements(By.CSS_SELECTOR, 'a[href*="/contents/"]')
            
            for elem in elements:
                url = elem.get_attribute('href')
                # ç®€å•çš„è¿‡æ»¤ï¼Œç¡®ä¿æ˜¯å†…å®¹é“¾æ¥
                if url and '/contents/' in url and url not in extracted_urls:
                    # æ¸…ç† URL (ç§»é™¤æŸ¥è¯¢å‚æ•°)
                    clean_url = url.split('?')[0]
                    # ç¡®ä¿æ˜¯å°è¯´è¯¦æƒ…é¡µè€Œéç« èŠ‚é¡µ
                    if '/episodes/' not in clean_url:
                        extracted_urls.add(clean_url)
        except Exception as e:
            print(f"âš ï¸ æå–é“¾æ¥æ—¶å‡ºé”™: {e}")

        current_count = len(extracted_urls)
        print(f"  å½“å‰æ‰¾åˆ° {current_count} ä¸ªå”¯ä¸€å°è¯´...")

        if current_count >= count:
            print(f"âœ… å·²æ‰¾åˆ°è¶³å¤Ÿçš„å°è¯´ ({current_count} >= {count})")
            break
        # 2. æ»šåŠ¨é¡µé¢ (ä½¿ç”¨é”®ç›˜äº‹ä»¶)
        try:
            body = driver.find_element(By.TAG_NAME, 'body')
            
            # å°è¯•ç‚¹å‡» body ä»¥ç¡®ä¿è·å¾—ç„¦ç‚¹ (å¦‚æœä¸æ‹¦æˆª)
            try:
                body.click()
            except:
                pass

            # æ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸ºï¼šEnd é”®åˆ°åº•
            body.send_keys(Keys.END)
            time.sleep(1)
            
            # å†æŒ‰ä¸€æ¬¡ PageDown ç¡®ä¿åº•éƒ¨
            body.send_keys(Keys.PAGE_DOWN)
            
        except Exception as e:
            print(f"âš ï¸ æ»šåŠ¨å¤±è´¥: {e}")
            # é™çº§åˆ° JS æ»šåŠ¨
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

        # ç­‰å¾…åŠ è½½
        time.sleep(2)

        # 3. æ£€æŸ¥é«˜åº¦å˜åŒ–
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            consecutive_no_change += 1
            print(f"  âš ï¸ é¡µé¢é«˜åº¦æœªå˜åŒ– ({consecutive_no_change}/{max_consecutive_no_change})...")
            
            # å°è¯•å¼ºåˆ¶è§¦å‘ï¼šå‘ä¸Šæ»šåŠ¨ä¸€ç‚¹ç‚¹å†å‘ä¸‹ (Oscillation)
            if consecutive_no_change < max_consecutive_no_change:
                 try:
                    # ä½¿ç”¨ JS å›æ»š 1000pxï¼Œæ¨¡æ‹Ÿç”¨æˆ·å¾€å›çœ‹
                    driver.execute_script("window.scrollBy(0, -1000);")
                    time.sleep(1)
                    # å†æ»šåŠ¨åˆ°åº•éƒ¨
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                 except Exception as e:
                    print(f"  âš ï¸ é‡è¯•æ»šåŠ¨å¤±è´¥: {e}")
                 
                 time.sleep(2)
                 # æ›´æ–°é«˜åº¦
                 new_height = driver.execute_script("return document.body.scrollHeight")
            else:
                print(f"âš ï¸ å·²å¤šæ¬¡æ»šåŠ¨åˆ°åº•éƒ¨ä¸”æ— æ–°å†…å®¹ï¼Œåœæ­¢æ»šåŠ¨ã€‚")
                break
        else:
            consecutive_no_change = 0 # é‡ç½®è®¡æ•°å™¨
            last_height = new_height
            
        scroll_attempts += 1
    
    # è¿”å›å‰ N ä¸ª URL
    result_urls = sorted(list(extracted_urls))[:count]
    
    print(f"\nâœ… æˆåŠŸæå– {len(result_urls)} ä¸ªå°è¯´ URL")
    return result_urls


def main():
    parser = argparse.ArgumentParser(description='ä» Diritto æ¦œå•é¡µé¢æå–å°è¯´ URL')
    parser.add_argument(
        '--count',
        type=int,
        default=10,
        help='è¦æå–çš„å°è¯´æ•°é‡ (é»˜è®¤: 10)'
    )
    parser.add_argument(
        '--url',
        type=str,
        default='https://www.diritto.co.kr/explore/completed-or-published/bl?exploreSubMenu=Completed',
        help='æ¦œå•é¡µé¢ URL (é»˜è®¤: BLå®Œç»“æ¦œå•)'
    )
    
    args = parser.parse_args()
    
    # éªŒè¯æ•°é‡èŒƒå›´
    if args.count < 1 or args.count > 100:
        print("âŒ é”™è¯¯: æå–æ•°é‡å¿…é¡»åœ¨ 1-100 ä¹‹é—´")
        sys.exit(1)

    # æ¸…ç†æ—§çš„ç»“æœæ–‡ä»¶ï¼Œé˜²æ­¢å‰ç«¯è¯»å–åˆ°ç¼“å­˜
    output_file = "diritto_extracted_urls.json"
    if os.path.exists(output_file):
        try:
            os.remove(output_file)
            print(f"[ä¿¡æ¯] å·²æ¸…ç†æ—§çš„ç¼“å­˜æ–‡ä»¶: {output_file}")
        except Exception as e:
            print(f"[è­¦å‘Š] æ— æ³•æ¸…ç†æ—§æ–‡ä»¶: {e}")
    
    # è¿æ¥æµè§ˆå™¨
    driver = setup_driver()
    if not driver:
        sys.exit(1)
    
    try:
        # æå– URL
        urls = extract_novel_urls(driver, args.url, args.count)
        
        # è¾“å‡º JSON æ ¼å¼ï¼ˆä¾›å‰ç«¯ä½¿ç”¨ï¼‰
        result = {"urls": urls}
        print("\n" + "="*60)
        print("JSON è¾“å‡º:")
        print("="*60)
        print(json.dumps(result, ensure_ascii=False, indent=2))
        
        # ä¹Ÿä¿å­˜åˆ°æ–‡ä»¶ï¼ˆæ–¹ä¾¿æŸ¥çœ‹ï¼‰
        output_file = "diritto_extracted_urls.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
    except Exception as e:
        print(f"âŒ æå–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        print("\nâœ… ä»»åŠ¡å®Œæˆã€‚æµè§ˆå™¨ä¿æŒæ‰“å¼€çŠ¶æ€ã€‚")


if __name__ == "__main__":
    main()
