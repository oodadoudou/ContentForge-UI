import os
import sys
import time
import json
import shutil
import traceback

# Add the project root to sys.path to find backend module
# Assuming this script is at backend/downloaders/diritto/diritto_downloader.py
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
sys.path.insert(0, project_root)

# Add the direito directory to sys.path to find browser_launcher
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from backend.downloaders.diritto.browser_launcher import setup_driver_with_auto_launch
from backend.utils import get_default_work_dir

# --- è„šæœ¬æ ¸å¿ƒä»£ç  ---

def log(msg, level="INFO"):
    """Enhanced logging with flush=True for real-time output"""
    prefix = f"[{level}]"
    print(f"{prefix} {msg}", flush=True)

def setup_driver():
    """é…ç½®å¹¶è¿æ¥åˆ° Chrome æµè§ˆå™¨ï¼ˆè‡ªåŠ¨å¯åŠ¨ï¼‰"""
    return setup_driver_with_auto_launch()

def process_book(driver, start_url, download_path):
    """
    å¤„ç†å•æœ¬ä¹¦ç±çš„å®Œæ•´ä¸‹è½½æµç¨‹ï¼Œé‡‡ç”¨ä»ä¸»é¡µå¼€å§‹å¹¶æ»šåŠ¨åŠ è½½çš„ç­–ç•¥ã€‚
    """
    stats = {'skipped': 0, 'successful': 0, 'failed': 0, 'failed_items': []}
    
    try:
        # 1. ç¡®å®šä¹¦ç±çš„ä¸»é¡µURL
        is_chapter_url = "/episodes/" in start_url
        base_url = start_url.split('/episodes/')[0] if is_chapter_url else start_url.split('?')[0]
        base_url = base_url.rstrip('/')

        log(f"æ­£åœ¨è®¿é—®ä¹¦ç±ä¸»é¡µ: {base_url}")
        driver.get(base_url)
        wait = WebDriverWait(driver, 45)  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°45ç§’
        
        # 2. è·å–å°è¯´æ ‡é¢˜
        log("æ­£åœ¨ç­‰å¾…é¡µé¢åŠ è½½å¹¶è·å–å°è¯´æ ‡é¢˜...")
        
        # å°è¯•ä½¿ç”¨ Meta æ ‡ç­¾è·å–æ ‡é¢˜ (æ›´ç¨³å®š)
        novel_title = None
        
        try:
            # ç­–ç•¥1: og:title
            og_title = driver.find_elements(By.CSS_SELECTOR, 'meta[property="og:title"]')
            if og_title:
                novel_title = og_title[0].get_attribute('content')
                log(f"âœ… æ‰¾åˆ°å°è¯´æ ‡é¢˜ (Meta: og:title): {novel_title}")
            
            # ç­–ç•¥2: twitter:title
            if not novel_title:
                tw_title = driver.find_elements(By.CSS_SELECTOR, 'meta[name="twitter:title"]')
                if tw_title:
                    novel_title = tw_title[0].get_attribute('content')
                    log(f"âœ… æ‰¾åˆ°å°è¯´æ ‡é¢˜ (Meta: twitter:title): {novel_title}")

            # ç­–ç•¥3: document.title
            if not novel_title:
                doc_title = driver.title
                if doc_title:
                    # é€šå¸¸æ ¼å¼ä¸º "Title | Diritto" æˆ–ç±»ä¼¼ï¼Œéœ€æ¸…ç†
                    novel_title = doc_title.split('|')[0].strip()
                    log(f"âœ… æ‰¾åˆ°å°è¯´æ ‡é¢˜ (Document Title): {novel_title}")
            
            # ç­–ç•¥4: H1 æ ‡ç­¾ (ä½œä¸ºæœ€åçš„å¤‡é€‰)
            if not novel_title:
                h1_elements = driver.find_elements(By.TAG_NAME, 'h1')
                if h1_elements:
                    novel_title = h1_elements[0].text.strip()
                    log(f"âœ… æ‰¾åˆ°å°è¯´æ ‡é¢˜ (H1): {novel_title}")

        except Exception as e:
            log(f"âš ï¸ è·å–æ ‡é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {e}", level="WARN")

        # æ¸…ç†æ–‡ä»¶åéæ³•å­—ç¬¦
        if novel_title:
            original_title = novel_title
            novel_title = novel_title.replace('/', '_').replace('\\', '_').replace(':', 'ï¼š').replace('?', 'ï¼Ÿ').replace('*', '_').replace('"', '_').replace('<', '_').replace('>', '_').replace('|', '_')
            if original_title != novel_title:
                log(f"  (æ–‡ä»¶åå·²æ¸…ç†: {original_title} -> {novel_title})")
        
        if not novel_title:
            log("âš ï¸ è­¦å‘Š: æœªèƒ½è·å–å°è¯´æ ‡é¢˜ï¼Œä½¿ç”¨é»˜è®¤åç§°", level="WARN")
            novel_title = "æœªçŸ¥å°è¯´"
            
        log(f"ğŸ“˜ å°è¯´æ ‡é¢˜: {novel_title}")

        # 3. æ»šåŠ¨åˆ°åº•éƒ¨ä»¥åŠ è½½æ‰€æœ‰ç« èŠ‚
        log("æ­£åœ¨è·å–ç« èŠ‚åˆ—è¡¨ (æ»šåŠ¨åŠ è½½)...")
        
        # æ»šåŠ¨åŠ è½½ç­–ç•¥ï¼Œå¢åŠ å°è¯•æ¬¡æ•°é™åˆ¶
        last_height = driver.execute_script("return document.body.scrollHeight")
        scroll_attempts = 0
        max_scroll_attempts = 10
        
        while scroll_attempts < max_scroll_attempts:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3)  # å¢åŠ ç­‰å¾…æ—¶é—´
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                log("âœ… å·²æ»šåŠ¨åˆ°åº•éƒ¨ï¼ŒåŠ è½½å®Œæˆã€‚")
                break
            last_height = new_height
            scroll_attempts += 1
            log(f"  æ»šåŠ¨ä¸­... ({scroll_attempts}/{max_scroll_attempts})")
        
        if scroll_attempts >= max_scroll_attempts:
            log("âš ï¸ è¾¾åˆ°æœ€å¤§æ»šåŠ¨å°è¯•æ¬¡æ•°ï¼Œåœæ­¢æ»šåŠ¨ã€‚", level="WARN")
        
        # 4. è·å–æ‰€æœ‰ç« èŠ‚é“¾æ¥ - è¿™é‡Œçš„é€»è¾‘å·²æ›´æ–°ä¸ºæ›´é²æ£’çš„"æœ€ä½³å®¹å™¨"æŸ¥æ‰¾ç­–ç•¥
        log("æ­£åœ¨åˆ†æé¡µé¢ç»“æ„ä»¥å®šä½ç« èŠ‚åˆ—è¡¨...")
        
        # ç­–ç•¥ï¼šæŸ¥æ‰¾é¡µé¢ä¸ŠåŒ…å«æœ€å¤šæœ‰æ•ˆç« èŠ‚é“¾æ¥çš„å®¹å™¨(ulæˆ–div)
        candidate_containers = driver.find_elements(By.TAG_NAME, "ul") + \
                               driver.find_elements(By.CSS_SELECTOR, "div[class*='list']")
        
        best_container = None
        max_valid_links = 0
        
        for container in candidate_containers:
            try:
                # å¿«é€Ÿæ£€æŸ¥å®¹å™¨å†…æ˜¯å¦æœ‰é“¾æ¥
                links = container.find_elements(By.TAG_NAME, "a")
                valid_count = 0
                for link in links:
                    href = link.get_attribute('href')
                    if href and ('/episodes/' in href or 'episode' in href):
                        valid_count += 1
                
                if valid_count > max_valid_links:
                    max_valid_links = valid_count
                    best_container = container
            except Exception:
                continue
        
        full_url_list = []
        
        # å¦‚æœæ‰¾åˆ°äº†åŒ…å«å¤šä¸ªé“¾æ¥çš„å®¹å™¨ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™å›é€€åˆ°å…¨æ–‡æœç´¢
        target_scope = best_container if (best_container and max_valid_links > 3) else driver
        scope_name = "æœ€ä½³åŒ¹é…å®¹å™¨" if (best_container and max_valid_links > 3) else "æ•´ä¸ªé¡µé¢(å›é€€æ¨¡å¼)"
        log(f"âœ… ä½¿ç”¨ {scope_name} è¿›è¡Œé“¾æ¥æå– (å‘ç° {max_valid_links if best_container else 0} ä¸ªæ½œåœ¨é“¾æ¥)")

        try:
            # è·å–èŒƒå›´å†…çš„æ‰€æœ‰é“¾æ¥
            all_links = target_scope.find_elements(By.TAG_NAME, "a")
            
            for link in all_links:
                href = link.get_attribute('href')
                text = link.text.strip()
                
                # æ ¸å¿ƒè¿‡æ»¤é€»è¾‘
                if href and ('/episodes/' in href or 'episode' in href):
                    # æ’é™¤"å…¬çŸ¥"(Notice)ç±»å‹çš„é“¾æ¥
                    if "ê³µì§€" in text:
                        # log(f"   (è·³è¿‡å…¬å‘Š: {text})")
                        continue
                        
                    full_url_list.append(href)
            
            # å»é‡å¹¶æ’åº
            full_url_list = sorted(list(set(full_url_list)))
            
        except Exception as e:
            log(f"âŒ æå–é“¾æ¥æ—¶å‘ç”Ÿé”™è¯¯: {e}", level="ERROR")

        if not full_url_list:
            log("âŒ é”™è¯¯: æœªèƒ½æ‰¾åˆ°ä»»ä½•ç« èŠ‚é“¾æ¥ã€‚", level="ERROR")
            return None, None, stats
            
        log(f"å…±æ‰¾åˆ° {len(full_url_list)} ä¸ªç« èŠ‚ã€‚")
        if len(full_url_list) > 0:
             log(f"   ğŸ”— é¦–ç« : {full_url_list[0]}")
             log(f"   ğŸ”— æœ«ç« : {full_url_list[-1]}")

        # 5. ç¡®å®šä¸‹è½½èµ·ç‚¹
        start_index = 0
        if is_chapter_url:
            try:
                clean_start_url = start_url.split('?')[0].rstrip('/')
                clean_full_url_list = [url.split('?')[0].rstrip('/') for url in full_url_list]
                start_index = clean_full_url_list.index(clean_start_url)
                log(f"âœ… æ‰¾åˆ°ä¸‹è½½èµ·ç‚¹ï¼Œå°†ä»ç¬¬ {start_index + 1} ç« å¼€å§‹å¤„ç†ã€‚")
            except ValueError:
                log(f"âš ï¸ è­¦å‘Š: æ‚¨è¾“å…¥çš„ç« èŠ‚URL {start_url} æœªåœ¨æœ€ç»ˆçš„ç›®å½•åˆ—è¡¨ä¸­æ‰¾åˆ°ã€‚å°†ä»ç¬¬ä¸€ç« å¼€å§‹å¤„ç†ã€‚", level="WARN")
        
        # åˆ›å»ºä»¥å°è¯´åå‘½åçš„ä¸»ç›®å½•åŠå­ç›®å½•ç»“æ„
        book_dir = os.path.join(download_path, novel_title)
        chapters_subdir = os.path.join(book_dir, "åˆ†å·")
        complete_txt_dir = os.path.join(book_dir, "å®Œæ•´txt")
        os.makedirs(chapters_subdir, exist_ok=True)
        os.makedirs(complete_txt_dir, exist_ok=True)
        log(f"æ‰€æœ‰æ–‡ä»¶å°†ä¿å­˜åœ¨: {book_dir}")
        log(f"  - åˆ†å·ç›®å½•: {chapters_subdir}")
        log(f"  - å®Œæ•´txtç›®å½•: {complete_txt_dir}")
        log(f"DEBUG: å®é™…ç»å¯¹è·¯å¾„å†™å…¥æµ‹è¯•: {os.path.abspath(chapters_subdir)}", level="INFO")
        
        # 6. å¾ªç¯ä¸‹è½½æ¯ä¸ªç« èŠ‚ï¼Œå¹¶åŠ å…¥é‡è¯•é€»è¾‘
        consecutive_failures = 0
        for i, url in enumerate(full_url_list[start_index:], start=start_index):
            chapter_number = i + 1
            log(f"--- æ­£åœ¨å¤„ç†ã€Š{novel_title}ã€‹- ç¬¬ {chapter_number} / {len(full_url_list)} ç«  ---")
            
            chapter_prefix = f"{str(chapter_number).zfill(4)}_"
            
            # æ£€æŸ¥åˆ†å·ç›®å½•ä¸­æ˜¯å¦å·²å­˜åœ¨æ­¤ç« èŠ‚
            existing_files = []
            if os.path.exists(chapters_subdir):
                existing_files = [f for f in os.listdir(chapters_subdir) if f.startswith(chapter_prefix)]

            if existing_files:
                existing_file_name = existing_files[0]
                log(f"âœ… æ£€æµ‹åˆ°æ–‡ä»¶ '{existing_file_name}'ï¼Œæœ¬ç« å·²ä¸‹è½½ï¼Œå°†è·³è¿‡ã€‚")
                stats['skipped'] += 1
                consecutive_failures = 0  # è§†ä¸ºæˆåŠŸä»¥é‡ç½®è®¡æ•°
                continue

            retries = 0
            MAX_RETRIES = 2
            download_successful = False
            
            while retries < MAX_RETRIES and not download_successful:
                try:
                    if retries > 0:
                        log(f"  - ç¬¬ {retries} æ¬¡é‡è¯•... URL: {url}", level="WARN")
                    else:
                        log(f"  - URL: {url}")
                        
                    driver.get(url)

                    # å°è¯•å¤šä¸ªå¯èƒ½çš„ç« èŠ‚æ ‡é¢˜é€‰æ‹©å™¨ (é¿å… hardcode hash)
                    chapter_title_selectors = [
                        'span[class*="css-p50amq"]',  # Diritto ç« èŠ‚æ ‡é¢˜ç¨³å®šå‰ç¼€
                        'h1[class*="title"]',         # å¤‡ç”¨é€‰æ‹©å™¨1
                        'h1',                         # é€šç”¨h1é€‰æ‹©å™¨
                        'h2',                         # å¤‡ç”¨h2é€‰æ‹©å™¨
                        '[class*="title"]'            # ä»»ä½•åŒ…å«titleçš„class
                    ]
                    
                    chapter_title = None
                    for selector in chapter_title_selectors:
                        try:
                            # å¿«é€Ÿæ£€æµ‹(2s)
                            chapter_title_element = WebDriverWait(driver, 2).until(EC.visibility_of_element_located((By.CSS_SELECTOR, selector)))
                            chapter_title = chapter_title_element.text.strip()
                            if chapter_title:  # ç¡®ä¿æ ‡é¢˜ä¸ä¸ºç©º
                                break
                        except (TimeoutException, Exception):
                            continue
                    
                    if not chapter_title:
                        chapter_title = f"ç¬¬{chapter_number}ç« "
                        log(f"  âš ï¸ æ— æ³•è·å–ç« èŠ‚æ ‡é¢˜ï¼Œä½¿ç”¨é»˜è®¤: {chapter_title}", level="WARN")
                    
                    # 6. è·å–ç« èŠ‚å†…å®¹
                    content_selectors = [
                        'div.ProseMirror',           # æœ€å¸¸è§çš„ProseMirrorå®¹å™¨
                        'div[class*="ProseMirror"]', # å®½æ³›åŒ¹é…
                        '.tiptap.ProseMirror',       
                        'div[contenteditable="false"]',
                        '.viewer-content',
                        'article',
                        '#viewer-content'
                        # ç§»é™¤ 'main' é€‰æ‹©å™¨ï¼Œå› ä¸ºå®ƒä¼šåŒ¹é…åˆ°é”™è¯¯é¡µé¢çš„æ•´é¡µå†…å®¹å¯¼è‡´False Positive
                    ]
                    
                    content = None
                    for selector in content_selectors:
                        try:
                            # å¿«é€Ÿæ£€æµ‹(2s)
                            content_container = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
                            
                            # ç­–ç•¥1: å°è¯•è·å–æ‰€æœ‰ p æ ‡ç­¾ (é€šå¸¸æ ¼å¼æ›´å¥½)
                            content_elements = content_container.find_elements(By.CSS_SELECTOR, 'p')
                            if content_elements:
                                content = "\n\n".join([p.text for p in content_elements if p.text.strip()])
                            
                            # ç­–ç•¥2: å¦‚æœæ²¡æœ‰ p æ ‡ç­¾æˆ–å†…å®¹ä¸ºç©ºï¼Œç›´æ¥è·å–å®¹å™¨æ–‡æœ¬ (innerText)
                            if not content or not content.strip():
                                content = content_container.get_attribute('innerText')
                                
                            if content and content.strip():  # ç¡®ä¿å†…å®¹ä¸ä¸ºç©º
                                # --- æ ¸å¿ƒæ ¡éªŒé€»è¾‘ ---
                                # æ£€æŸ¥æ˜¯å¦æå–åˆ°äº†é”™è¯¯æç¤ºä¿¡æ¯
                                if "íšŒì°¨ ë‚´ìš©ì„ ë³¼ ìˆ˜ ì—†ëŠ” ì‘í’ˆì´ì—ìš”" in content:
                                     raise ValueError("å†…å®¹æ— æ³•æŸ¥çœ‹ (å¯èƒ½éœ€è¦ç™»å½•æˆ–è´­ä¹°)")
                                
                                # æ£€æŸ¥å†…å®¹é•¿åº¦ (å¦‚æœå¤ªçŸ­ï¼Œææœ‰å¯èƒ½æ˜¯é”™è¯¯æç¤º)
                                if len(content.strip()) < 100:
                                    log(f"âš ï¸ æå–å†…å®¹è¿‡çŸ­ ({len(content.strip())} å­—ç¬¦)ï¼Œå¯èƒ½ä¸ºé”™è¯¯æç¤º: {content.strip()[:20]}...", level="WARN")
                                    
                                log(f"âœ… æ‰¾åˆ°ç« èŠ‚å†…å®¹ï¼Œä½¿ç”¨é€‰æ‹©å™¨: {selector}")
                                break
                        except (TimeoutException, Exception):
                            continue
                    
                    if not content or not content.strip():
                        # å°è¯•ä¿å­˜å‡ºé”™é¡µé¢çš„HTMLä»¥ä¾¿è°ƒè¯•
                        try:
                            debug_file = "Single_chapter_debug.html"
                            with open(debug_file, "w", encoding="utf-8") as f:
                                f.write(driver.page_source)
                            log(f"  âš ï¸ ä¿å­˜å‡ºé”™é¡µé¢æºç è‡³: {debug_file}", level="DEBUG")
                        except:
                            pass
                        raise ValueError("è·å–åˆ°çš„å†…å®¹ä¸ºç©ºï¼Œå¯èƒ½é¡µé¢ç»“æ„å·²å˜åŒ–ã€‚")

                    sanitized_title = chapter_title.replace('/', '_').replace('\\', '_').replace(':', 'ï¼š')
                    file_name = f"{chapter_prefix}{sanitized_title}.txt"
                    file_path = os.path.join(chapters_subdir, file_name)
                    
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(f"{chapter_title}\n\n")
                        f.write(content)
                    
                    log(f"  âœ… å·²ä¿å­˜è‡³åˆ†å·ç›®å½•: {file_name}")
                    stats['successful'] += 1
                    download_successful = True

                except Exception as e:
                    retries += 1
                    error_msg = str(e)
                    log(f"  - æŠ“å–æœ¬ç« æ—¶å‡ºé”™ (å°è¯• {retries}/{MAX_RETRIES}): {error_msg}", level="ERROR")
                    
                    # å¦‚æœæ˜¯TimeoutExceptionï¼Œæä¾›æ›´è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
                    if "TimeoutException" in error_msg or "timeout" in error_msg.lower():
                        log(f"  - è¶…æ—¶é”™è¯¯ï¼Œå¯èƒ½æ˜¯é¡µé¢åŠ è½½è¿‡æ…¢æˆ–å…ƒç´ é€‰æ‹©å™¨å·²å˜åŒ–", level="WARN")
                        log(f"  - å½“å‰é¡µé¢URL: {driver.current_url}", level="WARN")
                        try:
                            page_source_preview = driver.page_source[:500]
                            log(f"  - é¡µé¢æºç é¢„è§ˆ: {page_source_preview}...", level="DEBUG")
                        except:
                            log("  - æ— æ³•è·å–é¡µé¢æºç é¢„è§ˆ", level="DEBUG")
                    
                    if retries < MAX_RETRIES:
                        time.sleep(5)  # å¢åŠ é‡è¯•é—´éš”
                    else:
                        log(f"  âŒ æŠ“å–æœ¬ç« å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚", level="ERROR")
                        stats['failed'] += 1
                        stats['failed_items'].append({'url': url, 'error': error_msg})

            if download_successful:
                consecutive_failures = 0
            else:
                consecutive_failures += 1
                if consecutive_failures >= 2:
                    log("!"*60, level="ERROR")
                    log("âŒ é”™è¯¯: è¿ç»­ 2 ç« æå–å†…å®¹å¤±è´¥ï¼Œåœæ­¢ä¸‹è½½å½“å‰ä¹¦ç±ã€‚", level="ERROR")
                    log("âš ï¸ æç¤º: å¦‚æœè¿Ÿè¿Ÿæ— æ³•ä¸‹è½½ï¼Œè¯·åœ¨ç°åœ¨æ‰“å¼€çš„æµè§ˆå™¨é‡Œç™»å…¥å·²ç»æˆäººè®¤è¯è¿‡çš„è´¦å·ï¼Œç„¶åå†æ¬¡ä½¿ç”¨", level="WARN")
                    log("âš ï¸ æç¤º: å¦‚æœä¾ç„¶æ— æ³•ä¸‹è½½å¯èƒ½æ˜¯dirittoå®˜æ–¹é™æ—¶å…è´¹å·²ç»ç»“æŸ", level="WARN")
                    log("!"*60, level="ERROR")
                    stats['notes'] = "dirittoå®˜æ–¹å·²ç»å…³é—­é˜…è¯»/éœ€è¦ç™»å½•"
                    break

            time.sleep(2)
            
        return novel_title, book_dir, stats

    except Exception as e:
        log(f"âŒ åœ¨å¤„ç†ä¹¦ç± {start_url} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", level="FATAL")
        traceback.print_exc()
        return None, None, stats

def merge_chapters(novel_title, book_dir):
    """å°†åˆ†å·ç›®å½•ä¸­æ‰€æœ‰TXTæ–‡ä»¶æŒ‰é¡ºåºåˆå¹¶ï¼Œä¿å­˜åˆ°å®Œæ•´txtç›®å½•ã€‚å°äº3KBçš„æ–‡ä»¶å°†è¢«è·³è¿‡åˆå¹¶ã€‚"""
    chapters_subdir = os.path.join(book_dir, "åˆ†å·")
    complete_txt_dir = os.path.join(book_dir, "å®Œæ•´txt")
    merged_filename = os.path.join(complete_txt_dir, f"{novel_title}_å®Œæ•´.txt")
    
    log(f"ğŸ”„ å¼€å§‹åˆå¹¶æ‰€æœ‰ç« èŠ‚åˆ°ä¸€ä¸ªæ–‡ä»¶: {merged_filename}")
    
    try:
        if not os.path.exists(chapters_subdir):
            log(f"âš ï¸ è­¦å‘Š: ç›®å½• {chapters_subdir} ä¸å­˜åœ¨ï¼Œæ— æ³•åˆå¹¶ã€‚", level="WARN")
            return
        
        # è·å–åˆ†å·ç›®å½•ä¸­æ‰€æœ‰çš„ txt æ–‡ä»¶
        all_txt_files = sorted([f for f in os.listdir(chapters_subdir) if f.endswith('.txt') and os.path.isfile(os.path.join(chapters_subdir, f))])

        if not all_txt_files:
            log("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°å¯ä¾›åˆå¹¶çš„ç« èŠ‚æ–‡ä»¶ã€‚", level="WARN")
            return

        # ç­›é€‰å‡ºå¤§äºç­‰äº3KBçš„æ–‡ä»¶ç”¨äºåˆå¹¶
        files_to_merge = []
        for filename in all_txt_files:
            file_path = os.path.join(chapters_subdir, filename)
            # ä¿®æ”¹ï¼šå°†åˆ¤æ–­æ¡ä»¶ä» 800 å­—èŠ‚æ”¹ä¸º 3 KB (3 * 1024 bytes)
            if os.path.getsize(file_path) < 3 * 1024:
                log(f"  - [è·³è¿‡åˆå¹¶] æ–‡ä»¶ '{filename}' å°äº 3 KBï¼Œè§†ä¸ºéæ­£æ–‡å†…å®¹ã€‚", level="DEBUG")
            else:
                files_to_merge.append(filename)

        if not files_to_merge:
            log("âš ï¸ è­¦å‘Š: ç­›é€‰åæ²¡æœ‰ç¬¦åˆå¤§å°è¦æ±‚çš„ç« èŠ‚æ–‡ä»¶å¯ä¾›åˆå¹¶ã€‚", level="WARN")
        else:
            # ç¡®ä¿å®Œæ•´txtç›®å½•å­˜åœ¨
            os.makedirs(complete_txt_dir, exist_ok=True)
            
            with open(merged_filename, 'w', encoding='utf-8') as outfile:
                for i, filename in enumerate(files_to_merge):
                    file_path = os.path.join(chapters_subdir, filename)
                    with open(file_path, 'r', encoding='utf-8') as infile:
                        outfile.write(infile.read())
                    
                    if i < len(files_to_merge) - 1:
                        outfile.write("\n\n\n==========\n\n\n")
            
            log(f"âœ… åˆå¹¶å®Œæˆï¼å°è¯´å·²ä¿å­˜è‡³: {os.path.abspath(merged_filename)}")
            log(f"ğŸ“‚ ç« èŠ‚åˆ†å·æ–‡ä»¶ä¿ç•™åœ¨: {os.path.abspath(chapters_subdir)}")
        
    except Exception as e:
        log(f"âŒ åˆå¹¶æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}", level="ERROR")

def print_book_report(stats, novel_title):
    """æ‰“å°å•æœ¬ä¹¦ç±çš„æ‰§è¡ŒæŠ¥å‘Š"""
    log("="*40)
    log(f"ğŸ“‹ å•æœ¬æŠ¥å‘Š: {novel_title or 'æœªçŸ¥ä¹¦ç±'}")
    log("="*40)
    log(f"âœ… æˆåŠŸä¸‹è½½: {stats['successful']} ç« ")
    log(f"â­ï¸ è·³è¿‡ä¸‹è½½: {stats['skipped']} ç«  (å·²å­˜åœ¨)")
    log(f"âŒ ä¸‹è½½å¤±è´¥: {stats['failed']} ç« ")
    
    if 'notes' in stats:
        log(f"âš ï¸ çŠ¶æ€å¤‡æ³¨: {stats['notes']}", level="WARN")

    if stats['failed_items']:
        log("--- å¤±è´¥é¡¹ç›®è¯¦æƒ… ---", level="WARN")
        for item in stats['failed_items']:
            log(f"- URL: {item['url']}", level="WARN")
            if 'error' in item:
                 log(f"  åŸå› : {item['error']}", level="WARN")
    log("="*40)

def print_total_report(all_book_stats):
    """æ‰“å°æ‰€æœ‰ä»»åŠ¡çš„æ€»æŠ¥å‘Š"""
    total_stats = {
        'books_processed': len(all_book_stats),
        'books_completed_successfully': 0,
        'books_with_failures': 0,
        'books_aborted': 0,
        'total_successful': 0,
        'total_skipped': 0,
        'total_failed': 0,
    }

    for stats in all_book_stats:
        total_stats['total_successful'] += stats['successful']
        total_stats['total_skipped'] += stats['skipped']
        total_stats['total_failed'] += stats['failed']
        
        if 'notes' in stats and ("åœæ­¢ä¸‹è½½" in stats.get('notes', '') or "å…³é—­å…è´¹" in stats.get('notes', '')):
             total_stats['books_aborted'] += 1
        elif stats['failed'] > 0:
            total_stats['books_with_failures'] += 1
        else:
            total_stats['books_completed_successfully'] += 1

    log("#"*50)
    log("ğŸ“Š æ‰€æœ‰ä»»åŠ¡æ€»æŠ¥å‘Š")
    log("#"*50)
    log(f"å¤„ç†ä¹¦ç±æ€»æ•°: {total_stats['books_processed']}")
    log(f"âœ… å®Œç¾å®Œæˆçš„ä¹¦ç±: {total_stats['books_completed_successfully']}")
    log(f"âš ï¸ éƒ¨åˆ†å¤±è´¥çš„ä¹¦ç±: {total_stats['books_with_failures']}")
    log(f"â›” ä¸¥é‡é”™è¯¯/ä¸­æ–­çš„ä¹¦ç±: {total_stats['books_aborted']}")
    log("-" * 20)
    log(f"æ€»è®¡æˆåŠŸä¸‹è½½ç« èŠ‚: {total_stats['total_successful']}")
    log(f"æ€»è®¡è·³è¿‡ç« èŠ‚: {total_stats['total_skipped']}")
    log(f"æ€»è®¡å¤±è´¥ç« èŠ‚: {total_stats['total_failed']}")
    
    if total_stats['books_aborted'] > 0:
        log("-" * 20)
        log("!! æ³¨æ„ !! æœ‰ä¹¦ç±å› è¿ç»­å¤±è´¥è€Œä¸­æ–­ä¸‹è½½ã€‚", level="WARN")
        log("å¯èƒ½åŸå› : 1. Dirittoå®˜æ–¹é™æ—¶å…è´¹ç»“æŸ 2. æœªç™»å½•è´¦å·æˆ–Cookieå¤±æ•ˆ", level="WARN")
        log("è¯·å°è¯•åœ¨æ‰“å¼€çš„æµè§ˆå™¨ä¸­ç™»å½•è´¦å·åé‡è¯•ã€‚", level="WARN")
        
    log("#"*50)

if __name__ == "__main__":
    MAX_CONSECUTIVE_FAILURES = 2
    
    # --- 1. å‚æ•°è§£æ ---
    # ç®€å•è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œæ”¯æŒ output å’Œ urls
    output_dir = None
    url_list = []
    
    args = sys.argv[1:]
    i = 0
    while i < len(args):
        arg = args[i]
        if arg == '--urls':
            if i + 1 < len(args):
                val = args[i+1]
                try:
                    url_list = json.loads(val)
                except:
                    url_list = [u.strip() for u in val.split(',') if u.strip()]
                i += 2
            else:
                log("âŒ é”™è¯¯: --urls å‚æ•°ç¼ºå°‘å€¼", level="ERROR")
                sys.exit(1)
        elif arg == '--output':
            if i + 1 < len(args):
                output_dir = args[i+1]
                i += 2
            else:
                log("âŒ é”™è¯¯: --output å‚æ•°ç¼ºå°‘å€¼", level="ERROR")
                sys.exit(1)
        elif arg.startswith("http"):
            url_list.append(arg)
            i += 1
        else:
            i += 1
            
    # --- 2. ç¡®å®šä¸‹è½½ç›®å½• ---
    # ä¼˜å…ˆä½¿ç”¨ --outputï¼Œå¦åˆ™å¼ºåˆ¶ä½¿ç”¨ç³»ç»Ÿé…ç½®çš„é»˜è®¤å·¥ä½œç›®å½•
    if not output_dir:
        output_dir = get_default_work_dir()
        log(f"æœªæŒ‡å®šè¾“å‡ºç›®å½•ï¼Œä½¿ç”¨é»˜è®¤ä¸‹è½½è·¯å¾„: {output_dir}")
    else:
        log(f"ä½¿ç”¨æŒ‡å®šè¾“å‡ºç›®å½•: {output_dir}")

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    if not os.path.exists(output_dir):
        try:
            os.makedirs(output_dir)
            log(f"å·²åˆ›å»ºä¸‹è½½ç›®å½•: {output_dir}")
        except Exception as e:
            log(f"âŒ æ— æ³•åˆ›å»ºç›®å½• {output_dir}: {e}", level="ERROR")
            sys.exit(1)

    # --- 3. è·å–URL (äº¤äº’æ¨¡å¼) ---
    if not url_list:
        log("è¯·è¾“å…¥ä¸€ä¸ªæˆ–å¤šä¸ªDirittoå°è¯´URL (å¯åˆ†å¤šè¡Œç²˜è´´, è¾“å…¥å®ŒæˆåæŒ‰ä¸¤æ¬¡å›è½¦ç»“æŸ):")
        lines = []
        while True:
            try:
                line = input()
                if not line:
                    break
                lines.append(line)
            except EOFError:
                break
        urls_input = " ".join(lines)
        url_list = [url for url in urls_input.split() if url.startswith("http")]
    
    if not url_list:
        log("âŒ é”™è¯¯: æœªè¾“å…¥æœ‰æ•ˆçš„URLã€‚", level="ERROR")
    else:
        driver = setup_driver()

        if driver:
            all_book_stats = []
            try:
                # --- é¡ºåºå¤„ç†ä¹¦ç± ---
                for i, novel_url in enumerate(url_list):
                    log("#"*60)
                    log(f"# å¼€å§‹å¤„ç†ç¬¬ {i + 1} / {len(url_list)} æœ¬ä¹¦: {novel_url}")
                    log("#"*60)

                    novel_title, book_dir, book_stats = process_book(driver, novel_url, output_dir)
                    
                    if book_stats:
                        all_book_stats.append(book_stats)
                        print_book_report(book_stats, novel_title)

                    if novel_title and book_dir:
                        # cleanup logic: å¦‚æœä¸‹è½½å®Œå…¨å¤±è´¥ (0 æˆåŠŸï¼Œ0 è·³è¿‡)ï¼Œæ¸…ç†ç›®å½•
                        if book_stats and (book_stats['successful'] + book_stats['skipped'] == 0):
                            log(f"âš ï¸ã€Š{novel_title}ã€‹ä¸‹è½½å®Œå…¨å¤±è´¥ (0 æˆåŠŸï¼Œ0 è·³è¿‡)ï¼Œæ­£åœ¨æ¸…ç†ç›®å½•...", level="WARN")
                            try:
                                if os.path.exists(book_dir):
                                    shutil.rmtree(book_dir)
                                    log(f"âœ… å·²åˆ é™¤æ— æ•ˆç›®å½•: {book_dir}")
                            except Exception as e:
                                log(f"âŒ æ¸…ç†ç›®å½•å¤±è´¥: {e}", level="ERROR")
                        elif book_stats and book_stats['failed'] > 0:
                            log(f"âš ï¸ã€Š{novel_title}ã€‹æ£€æµ‹åˆ°ä¸‹è½½å¤±è´¥çš„é¡¹ç›®ï¼Œå·²è·³è¿‡æ–‡ä»¶åˆå¹¶ã€‚", level="WARN")
                            log(f"æºæ–‡ä»¶ä¿ç•™åœ¨ç›®å½•ä¸­: {os.path.abspath(book_dir)}")
                        else:
                            merge_chapters(novel_title, book_dir)
            finally:
                if all_book_stats:
                    print_total_report(all_book_stats)
                log("æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ã€‚æ‚¨å¯ä»¥æ‰‹åŠ¨å…³é—­æµè§ˆå™¨ã€‚")
