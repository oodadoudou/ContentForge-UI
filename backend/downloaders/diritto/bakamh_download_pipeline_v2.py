import os
import re
import json
import time
import shutil
import sys
import threading
from pathlib import Path
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import StaleElementReferenceException
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import pandas as pd
import concurrent.futures
import requests
from itertools import groupby
from PIL import Image, ImageDraw, ImageFont, ImageFile
import base64
import io
from collections import Counter

# --- Helper Functions ---
def load_config(default_url, default_path):
    config_file = 'manga_downloader_config.json'
    if os.path.exists(config_file):
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, KeyError): pass
    return {'url': default_url, 'path': default_path}

def save_config(data):
    with open('manga_downloader_config.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4)

def sanitize_for_filename(name):
    if not name: return "Untitled"
    name = re.sub(r'[<>:"/\\|?*]', '', name)
    name = name.replace(':', ' - ')
    name = name.strip().rstrip('. ')
    return " ".join(name.split())

def parse_chapter_selection(selection_str, max_chapters):
    if selection_str.lower() == 'all':
        return list(range(1, max_chapters + 1))
    indices = set()
    for part in selection_str.split(','):
        part = part.strip()
        if '-' in part:
            try:
                start, end = map(int, part.split('-'))
                indices.update(range(start, end + 1))
            except ValueError: pass
        else:
            try:
                indices.add(int(part))
            except ValueError: pass
    return sorted([i for i in indices if 1 <= i <= max_chapters])

def get_timed_input(prompt, timeout=30):
    sys.stdout.write(prompt); sys.stdout.flush()
    input_str = [None]
    def read_input(target): target[0] = sys.stdin.readline().strip()
    thread = threading.Thread(target=read_input, args=(input_str,))
    thread.daemon = True; thread.start(); thread.join(timeout)
    if thread.is_alive():
        print("\n[!] è¾“å…¥è¶…æ—¶ï¼Œä½¿ç”¨é»˜è®¤å€¼ã€‚")
        return None
    return input_str[0]

class ErrorTracker:
    def __init__(self):
        self.warnings = []
        self.errors = []
        self.manga_path = None
        self.failed_chapters = set()  # è®°å½•å¤±è´¥çš„ç« èŠ‚

    def add_warning(self, chapter_name, message):
        self.warnings.append({'chapter': chapter_name, 'message': message})

    def add_error(self, chapter_name, error):
        self.errors.append({'chapter': chapter_name, 'error': str(error)})
        self.failed_chapters.add(chapter_name)

    def set_manga_path(self, path):
        self.manga_path = path

    def _categorize_errors(self):
        """å°†é”™è¯¯æŒ‰ç±»å‹åˆ†ç±»ç»Ÿè®¡"""
        error_categories = {
            'ç½‘ç»œè¶…æ—¶': [],
            'è¿æ¥ä¸­æ–­': [],
            'ä»£ç†é”™è¯¯': [],
            'å…¶ä»–é”™è¯¯': []
        }
        
        for error in self.errors:
            error_msg = error['error'].lower()
            chapter = error['chapter']
            
            if 'read timed out' in error_msg or 'timeout' in error_msg:
                error_categories['ç½‘ç»œè¶…æ—¶'].append(chapter)
            elif 'connection aborted' in error_msg or 'connection reset' in error_msg:
                error_categories['è¿æ¥ä¸­æ–­'].append(chapter)
            elif 'proxy' in error_msg or 'unable to connect to proxy' in error_msg:
                error_categories['ä»£ç†é”™è¯¯'].append(chapter)
            else:
                error_categories['å…¶ä»–é”™è¯¯'].append(chapter)
        
        return error_categories

    def _clean_failed_chapters(self, progress_data):
        """æ¸…ç†å¤±è´¥ç« èŠ‚ï¼šé‡ç½®çŠ¶æ€ä¸ºpendingå¹¶åˆ é™¤æ–‡ä»¶å¤¹"""
        if not self.manga_path or not self.failed_chapters:
            return
        
        print(f"\n[*] æ­£åœ¨æ¸…ç† {len(self.failed_chapters)} ä¸ªå¤±è´¥ç« èŠ‚...")
        
        manga_title = self.manga_path.name
        cleaned_count = 0
        
        for chapter_name in self.failed_chapters:
            try:
                # é‡ç½®ç« èŠ‚çŠ¶æ€ä¸ºpending
                if manga_title in progress_data and 'chapters' in progress_data[manga_title]:
                    if chapter_name in progress_data[manga_title]['chapters']:
                        progress_data[manga_title]['chapters'][chapter_name]['status'] = 'pending'
                        print(f"    [âœ“] å·²é‡ç½®ç« èŠ‚çŠ¶æ€: {chapter_name}")
                
                # åˆ é™¤ç« èŠ‚æ–‡ä»¶å¤¹
                chapter_folder = self.manga_path / sanitize_for_filename(chapter_name)
                if chapter_folder.exists() and chapter_folder.is_dir():
                    shutil.rmtree(chapter_folder)
                    print(f"    [âœ“] å·²åˆ é™¤æ–‡ä»¶å¤¹: {chapter_name}")
                    cleaned_count += 1
                    
            except Exception as e:
                print(f"    [!] æ¸…ç†ç« èŠ‚ {chapter_name} æ—¶å‡ºé”™: {e}")
        
        # ä¿å­˜æ›´æ–°åçš„è¿›åº¦
        if cleaned_count > 0:
            save_progress(self.manga_path, progress_data)
            print(f"    [âœ“] å·²æ¸…ç† {cleaned_count} ä¸ªç« èŠ‚æ–‡ä»¶å¤¹å¹¶é‡ç½®çŠ¶æ€")

    def print_summary(self, progress_data=None):
        print("\n" + "="*25 + " [ä»»åŠ¡æ€»ç»“æŠ¥å‘Š] " + "="*25)

        if self.manga_path:
            print(f"\n[+] æ¼«ç”»ä¿å­˜ç›®å½•:\n    -> {self.manga_path.resolve()}")
        
        if not self.warnings and not self.errors:
            print("\n[ğŸ‰] æ‰€æœ‰ä»»åŠ¡å‡å·²æˆåŠŸå®Œæˆï¼Œæœªå‘ç°ä»»ä½•é—®é¢˜ã€‚")
            print("\n" + "="*68)
            return

        if self.warnings:
            print("\n[!] è­¦å‘Š (è¯·æ‰‹åŠ¨æ£€æŸ¥ä»¥ä¸‹ç« èŠ‚):")
            for warning in self.warnings:
                print(f"    - ç« èŠ‚ [{warning['chapter']}]: {warning['message']}")

        if self.errors:
            print("\n[âœ—] å¤±è´¥æŠ¥å‘Š (ç®€æ´ç‰ˆ):")
            
            # æŒ‰é”™è¯¯ç±»å‹åˆ†ç±»æ˜¾ç¤º
            error_categories = self._categorize_errors()
            
            for category, chapters in error_categories.items():
                if chapters:
                    unique_chapters = list(set(chapters))  # å»é‡
                    print(f"\n    [{category}] å½±å“ç« èŠ‚ ({len(unique_chapters)}ä¸ª):")
                    for i, chapter in enumerate(unique_chapters):
                        if i < 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
                            print(f"      â€¢ {chapter}")
                        elif i == 5:
                            print(f"      â€¢ ... è¿˜æœ‰ {len(unique_chapters) - 5} ä¸ªç« èŠ‚")
                            break
            
            print(f"\n    [ğŸ“Š] æ€»è®¡å¤±è´¥ç« èŠ‚: {len(self.failed_chapters)} ä¸ª")
            
            # æ‰§è¡Œå¤±è´¥ç« èŠ‚æ¸…ç†
            if progress_data:
                self._clean_failed_chapters(progress_data)
        
        print("\n" + "="*68)


class MangaScraper:
    def __init__(self, driver):
        self.driver = driver

    def get_info_from_chapter_page(self, chapter_url):
        print(f"[*] æ­£åœ¨è®¿é—®: {chapter_url} ä»¥è·å–æ¼«ç”»ä¿¡æ¯...")
        try:
            self.driver.get(chapter_url)
            wait = WebDriverWait(self.driver, 20)
            breadcrumb_selector = ".c-breadcrumb ol.breadcrumb li a"
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, breadcrumb_selector)))
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            breadcrumb_links = soup.select(breadcrumb_selector)
            if len(breadcrumb_links) >= 2:
                manga_title = sanitize_for_filename(breadcrumb_links[-1].get_text(strip=True))
                print(f"[+] è·å–åˆ°æ¼«ç”»æ ‡é¢˜: {manga_title}")
                # The base URL for resolving relative chapter URLs
                base_url = self.driver.current_url
                return manga_title, base_url
        except Exception as e:
            print(f"[!] ä»ç« èŠ‚é¡µé¢è·å–ä¿¡æ¯å¤±è´¥: {e}")
        return None, None

    def get_chapters_from_dropdown(self, base_url):
        try:
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            chapters = []
            seen_urls = set()
            chapter_options = soup.select('select.single-chapter-select option')
            
            # Use a temporary list to add chapters in order, then reverse at the end
            temp_chapters = []
            for option in chapter_options:
                url = option.get('data-redirect')
                name = sanitize_for_filename(option.get_text(strip=True))
                
                if url and name:
                    # Resolve relative URLs to be absolute
                    full_url = urljoin(base_url, url)
                    if full_url not in seen_urls:
                        temp_chapters.append({'url': full_url, 'name': name})
                        seen_urls.add(full_url)
            
            # Reverse to get chronological order (Chapter 1, 2, 3...)
            chapters = list(reversed(temp_chapters))
            
            print(f"[+] æˆåŠŸè§£æåˆ° {len(chapters)} ä¸ªå”¯ä¸€ç« èŠ‚ã€‚")
            return chapters
        except Exception as e:
            print(f"[!] ä»ä¸‹æ‹‰èœå•è·å–ç« èŠ‚åˆ—è¡¨å¤±è´¥: {e}")
            return []

class ChapterScanPipeline:
    def __init__(self, driver, max_refresh_attempts=2):
        self.driver = driver
        self.scraper = MangaScraper(driver=driver)
        self.max_refresh_attempts = max_refresh_attempts
    
    def run_scan(self, manga_url):
        manga_title, base_url = self.scraper.get_info_from_chapter_page(manga_url)
        if not manga_title: return [], "Unknown Manga", None
        chapters = self.scraper.get_chapters_from_dropdown(base_url)
        for i, c in enumerate(chapters): c['index'] = i + 1
        return chapters, manga_title, base_url

    def _scroll_to_bottom_and_wait(self):
        print("    [i] æ­£åœ¨æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ‰€æœ‰å›¾ç‰‡...")
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        scroll_attempts = 0
        max_scroll_attempts = 50  # å¢åŠ æœ€å¤§æ»šåŠ¨æ¬¡æ•°
        
        for scroll_attempts in range(max_scroll_attempts):
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3)  # å¢åŠ æ¯æ¬¡æ»šåŠ¨åçš„ç­‰å¾…æ—¶é—´ä»2ç§’åˆ°3ç§’
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡æ­£åœ¨åŠ è½½
            try:
                loading_images = self.driver.find_elements(By.CSS_SELECTOR, "img[src*='loading'], img[data-src], img[src='']")
                if loading_images:
                    print(f"    [i] æ£€æµ‹åˆ° {len(loading_images)} ä¸ªå›¾ç‰‡æ­£åœ¨åŠ è½½ï¼Œç»§ç»­ç­‰å¾…...")
                    time.sleep(5)  # é¢å¤–ç­‰å¾…5ç§’è®©å›¾ç‰‡åŠ è½½å®Œæˆ
            except:
                pass
            
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                # å†æ¬¡æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å›¾ç‰‡åœ¨åŠ è½½
                try:
                    loading_images = self.driver.find_elements(By.CSS_SELECTOR, "img[src*='loading'], img[data-src], img[src='']")
                    if not loading_images:
                        break
                    else:
                        print(f"    [i] ä»æœ‰ {len(loading_images)} ä¸ªå›¾ç‰‡åœ¨åŠ è½½ï¼Œç»§ç»­ç­‰å¾…...")
                        time.sleep(3)
                except:
                    break
            last_height = new_height
        
        # æœ€ç»ˆç­‰å¾…ï¼Œç¡®ä¿æ‰€æœ‰å›¾ç‰‡éƒ½åŠ è½½å®Œæˆ
        print("    [i] é¡µé¢æ»šåŠ¨å®Œæˆï¼Œç­‰å¾…å›¾ç‰‡æœ€ç»ˆåŠ è½½...")
        time.sleep(8)  # å¢åŠ æœ€ç»ˆç­‰å¾…æ—¶é—´
        
        # å†æ¬¡æ£€æŸ¥å¹¶ç­‰å¾…ä»»ä½•å‰©ä½™çš„åŠ è½½å›¾ç‰‡
        try:
            loading_images = self.driver.find_elements(By.CSS_SELECTOR, "img[src*='loading'], img[data-src], img[src='']")
            if loading_images:
                print(f"    [i] æœ€ç»ˆç­‰å¾… {len(loading_images)} ä¸ªå›¾ç‰‡åŠ è½½å®Œæˆ...")
                time.sleep(10)  # é¢å¤–ç­‰å¾…10ç§’
        except:
            pass
            
        print("    [âœ“] é¡µé¢å·²æ»šåŠ¨åˆ°åº•éƒ¨ï¼Œå›¾ç‰‡åŠ è½½ç­‰å¾…å®Œæˆã€‚")

    def scan_images_on_page(self):
        self._scroll_to_bottom_and_wait()
        print("    [i] æ­£åœ¨æ™ºèƒ½æ‰«æé¡µé¢ä»¥æå–æ¼«ç”»å›¾ç‰‡å…ƒç´ ...")

        # å°è¯•å¤šæ¬¡æ‰«æï¼Œæ¯æ¬¡é—´éš”ç­‰å¾…
        max_retries = 3
        for retry in range(max_retries):
            if retry > 0:
                print(f"    [i] ç¬¬ {retry + 1} æ¬¡é‡è¯•æ‰«æ...")
                time.sleep(5)  # é‡è¯•å‰ç­‰å¾…5ç§’
            
            # --- New, more robust selection strategy based on user feedback ---
            primary_selector = ".reading-content img.wp-manga-chapter-img"
            img_elements = []
            try:
                img_elements = self.driver.find_elements(By.CSS_SELECTOR, primary_selector)
            except Exception as e:
                print(f"    [!] æŸ¥æ‰¾ä¸»é€‰æ‹©å™¨ '{primary_selector}' æ—¶å‡ºé”™: {e}")

            if img_elements:
                print(f"    [+] ä½¿ç”¨ä¸»é€‰æ‹©å™¨ '{primary_selector}' æ‰¾åˆ° {len(img_elements)} ä¸ªå›¾ç‰‡å…ƒç´ ã€‚")
                break
            else:
                print(f"    [!] ç¬¬ {retry + 1} æ¬¡å°è¯•ï¼šä¸»é€‰æ‹©å™¨æœªæ‰¾åˆ°å›¾ç‰‡ï¼Œå›é€€åˆ°é€šç”¨å®¹å™¨æ‰«æ...")
                container_selectors = [".reading-content", ".read-container", "div.chapter-c", ".chapter-content", ".manga-reader"]
                for selector in container_selectors:
                    try:
                        container = self.driver.find_element(By.CSS_SELECTOR, selector)
                        elements = container.find_elements(By.TAG_NAME, 'img')
                        if elements:
                            print(f"    [+] åœ¨å®¹å™¨ '{selector}' ä¸­æ‰¾åˆ° {len(elements)} ä¸ªå›¾ç‰‡å…ƒç´ ã€‚")
                            img_elements = elements
                            break
                    except:
                        continue
                
                if img_elements:
                    break
        
        if not img_elements:
            print("    [!] ç»è¿‡å¤šæ¬¡å°è¯•ï¼Œé¡µé¢ä¸Šä»æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡å…ƒç´ ã€‚")
            return []

        # --- Final, simplified extraction. We trust the selectors and do not filter by visibility/size. ---
        final_images = []
        for i, element in enumerate(img_elements):
            try:
                img_data = element.get_attribute('src') or element.get_attribute('data-src')
                # Basic sanity check for valid image data/URL.
                if img_data and 'gif' not in img_data and 'data:image/svg+xml' not in img_data:
                    # We trust the selector and no longer check .is_displayed() or .size, as they are unreliable.
                    final_images.append({'element': element, 'index': i, 'size': {}, 'data': img_data})
            except StaleElementReferenceException:
                continue
        
        print(f"    [+] æœ€ç»ˆæå–åˆ° {len(final_images)} ä¸ªæœ‰æ•ˆæ¼«ç”»å›¾ç‰‡ã€‚")
        return final_images

    def scan_images_with_refresh(self, chapter_url):
        """æ‰«æå›¾ç‰‡ï¼Œå¦‚æœå¤±è´¥åˆ™è‡ªåŠ¨åˆ·æ–°é¡µé¢é‡è¯•"""
        for refresh_attempt in range(self.max_refresh_attempts + 1):
            if refresh_attempt > 0:
                print(f"    [ğŸ”„] ç¬¬ {refresh_attempt}/{self.max_refresh_attempts} æ¬¡åˆ·æ–°é¡µé¢é‡è¯•...")
                try:
                    self.driver.refresh()
                    print(f"    [i] é¡µé¢å·²åˆ·æ–°ï¼Œç­‰å¾…é‡æ–°åŠ è½½...")
                    time.sleep(5)  # åˆ·æ–°åç­‰å¾…é¡µé¢é‡æ–°åŠ è½½
                except Exception as e:
                    print(f"    [!] é¡µé¢åˆ·æ–°å¤±è´¥: {e}")
                    continue
            
            print(f"    [i] ç¬¬ {refresh_attempt + 1}/{self.max_refresh_attempts + 1} æ¬¡æ‰«æå›¾ç‰‡...")
            infos = self.scan_images_on_page()
            if infos:
                if refresh_attempt > 0:
                    print(f"    [âœ“] ç¬¬ {refresh_attempt + 1} æ¬¡å°è¯•æˆåŠŸæ‰¾åˆ°å›¾ç‰‡ï¼")
                return infos
            else:
                print(f"    [!] ç¬¬ {refresh_attempt + 1} æ¬¡å°è¯•æœªæ‰¾åˆ°å›¾ç‰‡")
                if refresh_attempt < self.max_refresh_attempts:
                    print(f"    [i] å‡†å¤‡åˆ·æ–°é¡µé¢è¿›è¡Œç¬¬ {refresh_attempt + 2} æ¬¡å°è¯•...")
                    time.sleep(3)  # åˆ·æ–°å‰ç­‰å¾…3ç§’
        
        print(f"    [!] ç»è¿‡ {self.max_refresh_attempts + 1} æ¬¡å°è¯•ï¼ˆåŒ…æ‹¬ {self.max_refresh_attempts} æ¬¡åˆ·æ–°ï¼‰ï¼Œä»æ— æ³•æ‰¾åˆ°å›¾ç‰‡")
        return []

class ImageProcessor:
    def analyze_image_layout(self, image_files):
        # é»˜è®¤è¿”å›å•åˆ—å¸ƒå±€
        return {'layout': 'vertical', 'cols': 1, 'direction': 'ltr'}

    def stitch_image_tiles(self, image_files, output_path_base, stitch_info, max_height_px=10000):
        # å•åˆ—å¸ƒå±€ä¸éœ€è¦æ‹¼æ¥ï¼Œç›´æ¥è¿”å›æˆåŠŸ
        return True, "å•åˆ—å¸ƒå±€ï¼Œæ— éœ€æ‹¼æ¥ã€‚", []

    def remove_original_images(self, files):
        # ä¸åˆ é™¤åŸå§‹å›¾ç‰‡
        pass

class DownloadPipeline:
    def __init__(self, scanner, processor, tracker):
        self.scanner = scanner
        self.processor = processor
        self.tracker = tracker
        self.enable_processing = True # This will be set by the main function
        self.errors = []
    
    def download_image(self, data, path, session, max_retries=3):
        for attempt in range(max_retries):
            try:
                if data.startswith('data:image'):
                    header, encoded = data.split(',', 1)
                    encoded += '=' * (-len(encoded) % 4)
                    path.write_bytes(base64.b64decode(encoded))
                    return {'status': 'success', 'path': path}
                else:
                    r = session.get(data, stream=True, timeout=20)
                    r.raise_for_status()
                    with open(path, 'wb') as f:
                        for chunk in r.iter_content(8192): f.write(chunk)
                    return {'status': 'success', 'path': path}
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"        [!] å›¾ç‰‡ä¸‹è½½å¤±è´¥ï¼Œç¬¬ {attempt + 1} æ¬¡é‡è¯•: {str(e)[:50]}...")
                    time.sleep(2)  # é‡è¯•å‰ç­‰å¾…2ç§’
                else:
                    return {'status': 'failed', 'error': str(e)}
        return {'status': 'failed', 'error': 'Max retries exceeded'}

    def process_chapters(self, chapters, manga_path, base_url, progress_data):
        session = requests.Session()
        session.headers.update({'Referer': base_url}) 

        for chapter in chapters:
            chapter_display_name = f"{chapter['index']:03d} - {chapter['name']}"
            file_safe_chapter_name = f"{chapter['index']:03d}_{re.sub(r'[^a-zA-Z0-9]+', '', chapter['name'])}"
            
            # --- New JSON-based completion check ---
            if manga_path.name in progress_data and \
               chapter_display_name in progress_data[manga_path.name]['chapters'] and \
               progress_data[manga_path.name]['chapters'][chapter_display_name]['status'] == 'completed':
                print(f"\n[*] ({chapter['index']}/{len(chapters)}) [âœ“] å·²åœ¨ info.json ä¸­æ ‡è®°ä¸ºå®Œæˆï¼Œè·³è¿‡: {chapter_display_name}")
                continue

            print(f"\n[*] ({chapter['index']}/{len(chapters)}) æ£€æŸ¥: {chapter_display_name}")

            path = manga_path / chapter_display_name

            try:
                self.scanner.driver.get(chapter['url'])
                # ç­‰å¾…é¡µé¢åŸºæœ¬å…ƒç´ åŠ è½½
                time.sleep(3)
                infos = self.scanner.scan_images_with_refresh(chapter['url'])

                if not infos:
                    msg = "ç»è¿‡å¤šæ¬¡åˆ·æ–°å°è¯•åä»æœªæ‰¾åˆ°å›¾ç‰‡ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–é¡µé¢ç»“æ„å˜åŒ–ã€‚"
                    print(f"    [!] {msg}")
                    self.tracker.add_error(chapter_display_name, msg)
                    # åˆ é™¤ç« èŠ‚æ–‡ä»¶å¤¹å¹¶é‡ç½®çŠ¶æ€
                    if path.exists() and path.is_dir():
                        shutil.rmtree(path)
                    if manga_path.name in progress_data and 'chapters' in progress_data[manga_path.name]:
                        progress_data[manga_path.name]['chapters'][chapter_display_name]['status'] = 'pending'
                    continue

                if len(infos) < 20:
                    msg = f"ä»…æ‰¾åˆ° {len(infos)} å¼ å›¾ç‰‡ï¼Œå¯èƒ½ä¸å®Œæ•´ã€‚"
                    print(f"    [!] {msg}")
                    self.tracker.add_warning(chapter_display_name, msg)
                
                path.mkdir(exist_ok=True, parents=True)
                
                paths, failed = [], []
                with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
                    future_map = {executor.submit(self.download_image, info['data'], path / f"{info['index']:04d}.jpg", session): info for info in infos}
                    for future in concurrent.futures.as_completed(future_map):
                        res, info = future.result(), future_map[future]
                        if res['status'] == 'success':
                            paths.append(res['path'])
                        else:
                            failed.append(info)
                
                print(f"    [i] ä¸‹è½½å®Œæˆï¼ˆå·²é‡è¯•æœ€å¤š3æ¬¡ï¼‰ã€‚æˆåŠŸ {len(paths)}ï¼Œå¤±è´¥ {len(failed)}ã€‚")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹è½½å¤±è´¥çš„å›¾ç‰‡
                if failed:
                    print(f"    [!] æœ‰ {len(failed)} å¼ å›¾ç‰‡ä¸‹è½½å¤±è´¥ï¼ˆå·²é‡è¯•3æ¬¡ï¼‰ï¼Œæ ‡è®°ç« èŠ‚ä¸ºå¤±è´¥å¹¶æ¸…ç†...")
                    # è®°å½•å¤±è´¥çš„å›¾ç‰‡
                    for info in failed:
                        self.tracker.add_error(chapter_display_name, f"å›¾ç‰‡ä¸‹è½½å¤±è´¥ï¼ˆå·²é‡è¯•3æ¬¡ï¼‰: {info.get('data', 'unknown')}")
                    
                    # åˆ é™¤ç« èŠ‚æ–‡ä»¶å¤¹å¹¶é‡ç½®çŠ¶æ€
                    if path.exists() and path.is_dir():
                        shutil.rmtree(path)
                    if manga_path.name in progress_data and 'chapters' in progress_data[manga_path.name]:
                        progress_data[manga_path.name]['chapters'][chapter_display_name]['status'] = 'pending'
                    continue
                
                # åªæœ‰å½“æ‰€æœ‰å›¾ç‰‡éƒ½æˆåŠŸä¸‹è½½æ—¶æ‰æ ‡è®°ä¸ºå®Œæˆ
                if len(paths) == len(infos):
                    print(f"    [âœ“] æ‰€æœ‰ {len(paths)} å¼ å›¾ç‰‡ä¸‹è½½æˆåŠŸï¼Œç« èŠ‚å®Œæˆã€‚")
                    update_progress(manga_path, chapter_display_name, 'completed', progress_data)
                    save_progress(manga_path, progress_data)
                else:
                    print(f"    [!] å›¾ç‰‡ä¸‹è½½ä¸å®Œæ•´ï¼šæœŸæœ› {len(infos)} å¼ ï¼Œå®é™… {len(paths)} å¼ ")
                    print(f"    [!] æ ‡è®°ç« èŠ‚ä¸ºå¤±è´¥å¹¶æ¸…ç†...")
                    # è®°å½•é”™è¯¯
                    self.tracker.add_error(chapter_display_name, f"å›¾ç‰‡ä¸‹è½½ä¸å®Œæ•´ï¼šæœŸæœ› {len(infos)} å¼ ï¼Œå®é™… {len(paths)} å¼ ")
                    
                    # åˆ é™¤ç« èŠ‚æ–‡ä»¶å¤¹å¹¶é‡ç½®çŠ¶æ€
                    if path.exists() and path.is_dir():
                        shutil.rmtree(path)
                    if manga_path.name in progress_data and 'chapters' in progress_data[manga_path.name]:
                        progress_data[manga_path.name]['chapters'][chapter_display_name]['status'] = 'pending'
                        
            except Exception as e:
                print(f"    [!] å¤„ç†ç« èŠ‚æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                self.tracker.add_error(chapter_display_name, str(e))
                # åˆ é™¤ç« èŠ‚æ–‡ä»¶å¤¹å¹¶é‡ç½®çŠ¶æ€
                if path.exists() and path.is_dir():
                    shutil.rmtree(path)
                if manga_path.name in progress_data and 'chapters' in progress_data[manga_path.name]:
                    progress_data[manga_path.name]['chapters'][chapter_display_name]['status'] = 'pending'

def update_progress(manga_path, chapter_name, status, progress_data):
    """Updates the status of a chapter in the progress data."""
    manga_title = manga_path.name
    if manga_title not in progress_data:
        progress_data[manga_title] = {'chapters': {}}
    
    progress_data[manga_title]['chapters'][chapter_name] = {'status': status}

def save_progress(manga_path, progress_data):
    """Saves the progress data to info.json file."""
    manga_title = manga_path.name
    if manga_title in progress_data:
        info_file = manga_path / 'info.json'
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(progress_data.get(manga_title, {}), f, indent=4, ensure_ascii=False)

def main():
    config = load_config('https://bakamh.com/manga/be-be/c-1/', 'manga_output')
    url = input(f"1. URL [{config['url']}]: ") or config['url']
    path_str = input(f"2. æ ¹ç›®å½• [{config['path']}]: ") or config['path']
    
    # ä½¿ç”¨è¶…æ—¶è¾“å…¥è·å–åˆ·æ–°æ¬¡æ•°
    max_refresh_input = get_timed_input(f"3. é¡µé¢åˆ·æ–°é‡è¯•æ¬¡æ•° [5] (5ç§’å†…è¾“å…¥ï¼Œè¶…æ—¶ä½¿ç”¨é»˜è®¤å€¼): ", 5)
    if max_refresh_input is None:
        max_refresh = 5
        print(f"[*] ä½¿ç”¨é»˜è®¤åˆ·æ–°æ¬¡æ•°: {max_refresh}")
    else:
        try:
            max_refresh = int(max_refresh_input)
        except ValueError:
            max_refresh = 5
            print(f"[!] è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤åˆ·æ–°æ¬¡æ•°: {max_refresh}")
    
    save_config({'url': url, 'path': path_str})

    driver = None
    tracker = ErrorTracker()
    progress_data = {}
    manga_path = None # Define here for finally block

    try:
        options = uc.ChromeOptions()
        options.add_argument('--no-sandbox')
        driver = uc.Chrome(options=options)
        
        scanner = ChapterScanPipeline(driver, max_refresh_attempts=max_refresh)
        chapters, title, base_url = scanner.run_scan(url)
        if not chapters: 
            tracker.add_error("åˆå§‹åŒ–", "æœªèƒ½è·å–ä»»ä½•ç« èŠ‚åˆ—è¡¨ã€‚")
            return

        manga_path = Path(path_str) / title
        manga_path.mkdir(exist_ok=True, parents=True)
        tracker.set_manga_path(manga_path)
        
        # Load or initialize progress from info.json
        info_file = manga_path / 'info.json'
        if info_file.exists():
            with open(info_file, 'r', encoding='utf-8') as f:
                try:
                    progress_data[title] = json.load(f)
                except json.JSONDecodeError:
                    pass # Will be handled below
        
        # Ensure base structure exists and add new chapters
        if title not in progress_data or 'chapters' not in progress_data[title]:
             progress_data[title] = {'title': title, 'url': base_url, 'chapters': {}}
        
        existing_chapters = progress_data[title]['chapters']
        for c in chapters:
            chapter_display_name = f"{c['index']:03d} - {c['name']}"
            if chapter_display_name not in existing_chapters:
                existing_chapters[chapter_display_name] = {'status': 'pending'}

        df_data = []
        for c in chapters:
            chapter_display_name = f"{c['index']:03d} - {c['name']}"
            status = progress_data.get(title, {}).get('chapters', {}).get(chapter_display_name, {}).get('status', 'pending')
            df_data.append({"åºå·": c['index'], "ç« èŠ‚å": c['name'], "çŠ¶æ€": status})
        
        df = pd.DataFrame(df_data)
        print(df.to_string(index=False))

        selection = get_timed_input("\né€‰æ‹©ç« èŠ‚ (e.g., 1, 3-5, all) [all]: ", 30) or 'all'
        
        to_dl = [chapters[i-1] for i in parse_chapter_selection(selection, len(chapters))]
        
        print(f"\n[*] å°†å¼€å§‹å¤„ç†å›¾ç‰‡ (å•åˆ—å¸ƒå±€ï¼Œä¿ç•™åŸå§‹å›¾ç‰‡)ã€‚")
        print(f"[*] é¡µé¢åˆ·æ–°é‡è¯•æ¬¡æ•°è®¾ç½®ä¸º: {max_refresh} æ¬¡")
        print(f"[*] å¦‚æœé¡µé¢åŠ è½½å¤±è´¥ï¼Œå°†è‡ªåŠ¨åˆ·æ–°æœ€å¤š {max_refresh} æ¬¡")
        proc = True
        
        pipeline = DownloadPipeline(scanner, ImageProcessor(), tracker)
        pipeline.enable_processing = proc # Set enable_processing for the pipeline
        pipeline.process_chapters(to_dl, manga_path, base_url, progress_data)

    except Exception as e:
        tracker.add_error("è‡´å‘½é”™è¯¯", str(e))
        print(f"å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        if driver: driver.quit()
        # Save final progress state
        if manga_path and progress_data:
            save_progress(manga_path, progress_data)

        tracker.print_summary(progress_data)

if __name__ == '__main__':
    main()
